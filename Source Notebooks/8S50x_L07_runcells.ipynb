{"cells": [{"cell_type": "markdown", "id": "f44134fe", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<hr style=\"height: 1px;\">\n", "<i>This notebook was authored by the 8.S50x Course Team, Copyright 2022 MIT All Rights Reserved.</i>\n", "<hr style=\"height: 1px;\">\n", "<br>\n", "\n", "<h1>Lesson 7: Correlations</h1>\n"]}, {"cell_type": "markdown", "id": "7da993b2", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_7_0'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L7.0 Overview</h2>\n"]}, {"cell_type": "markdown", "id": "5bf91bc0", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<h3>Navigation</h3>\n", "\n", "<table style=\"width:100%\">\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_7_1\">L7.1 Understanding Best Fit (Revisited)</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_7_1\">L7.1 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_7_2\">L7.2 Minimizing on a Surface (1D Scan)</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_7_2\">L7.2 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_7_3\">L7.3 Minimizing on a Surface (2D Scan)</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_7_3\">L7.3 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_7_4\">L7.4 Correlations Between Fit Parameters: Part 1</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_7_4\">L7.4 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_7_5\">L7.5 Correlations Between Fit Parameters: Part 2</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_7_5\">L7.5 Exercises</a></td>\n", "    </tr>\n", "</table>\n", "\n"]}, {"cell_type": "markdown", "id": "4105ab2e", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<h3>Importing Data (Colab Only)</h3>\n", "\n", "If you are in a Google Colab environment, run the cell below to import the data for this notebook. Otherwise, if you have downloaded the course repository, you do not have to run the cell below.\n", "\n", "See the source and attribution information below:\n", "\n", ">data: data/L05/events_a4_1space.dat, data/L05/events_a8_1space.dat <br>\n", ">source: https://www.auger.org/index.php/science/data <br>\n", ">source : https://arxiv.org/abs/1709.07321 <br>\n", ">attribution: Pierre Auger Collaboration, arXiv:1709.07321v1 <br>\n", ">license type: https://creativecommons.org/licenses/by-sa/4.0/ "]}, {"cell_type": "code", "execution_count": null, "id": "65a5b9ff", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.0-runcell00\n", "\n", "#importing data from git repository\n", "\n", "!git init\n", "!git remote add -f origin https://github.com/mitx-8s50/nb_LEARNER/\n", "!git config core.sparseCheckout true\n", "!echo 'data/L05' >> .git/info/sparse-checkout\n", "!git pull origin main"]}, {"cell_type": "code", "execution_count": null, "id": "7b8fed76", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.0-runcell01\n", "\n", "# for Colab users\n", "!pip install lmfit"]}, {"cell_type": "code", "execution_count": null, "id": "33156765", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.0-runcell02\n", "\n", "import numpy as np                 #https://numpy.org/doc/stable/\n", "from scipy import optimize as opt  #https://docs.scipy.org/doc/scipy/reference/optimize.html\n", "import matplotlib.pyplot as plt    #https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.html\n", "import math                        #https://docs.python.org/3/library/math.html\n", "import csv                         #https://docs.python.org/3/library/csv.html\n", "import lmfit                       #https://lmfit.github.io/lmfit-py/ \n", "import scipy.stats as stats        #https://docs.scipy.org/doc/scipy/reference/stats.html\n", "from scipy.optimize.zeros import RootResults        #https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.RootResults.html\n", "from scipy.optimize.optimize import OptimizeResult  # https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeResult.html\n", "from typing import Callable, Tuple #https://docs.python.org/3/library/typing.html#typing.Callable\n", "                                   #https://docs.python.org/3/library/typing.html\n", "import numpy.linalg as la          #https://docs.scipy.org/doc/scipy/reference/linalg.html\n", "from matplotlib.patches import Ellipse  # https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.patches.Ellipse.html\n"]}, {"cell_type": "code", "execution_count": null, "id": "67d4684e", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.0-runcell03\n", "\n", "#set plot resolution\n", "%config InlineBackend.figure_format = 'retina'\n", "\n", "#set default figure parameters\n", "plt.rcParams['figure.figsize'] = (9,6)\n", "\n", "medium_size = 12\n", "large_size = 15\n", "\n", "plt.rc('font', size=medium_size)          # default text sizes\n", "plt.rc('xtick', labelsize=medium_size)    # xtick labels\n", "plt.rc('ytick', labelsize=medium_size)    # ytick labels\n", "plt.rc('legend', fontsize=medium_size)    # legend\n", "plt.rc('axes', titlesize=large_size)      # axes title\n", "plt.rc('axes', labelsize=large_size)      # x and y labels\n", "plt.rc('figure', titlesize=large_size)    # figure title\n"]}, {"cell_type": "markdown", "id": "0ad89b66", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_7_1'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L7.1 Understanding Best Fit (Revisited)</h2>  \n", "\n", "| [Top](#section_7_0) | [Previous Section](#section_7_0) | [Exercises](#exercises_7_1) | [Next Section](#section_7_2) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "1c381a19", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.1-slides\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L07/slides_L07_01.html', width=970, height=550)"]}, {"cell_type": "code", "execution_count": null, "id": "b9d7a68b", "metadata": {"tags": ["learner", "py", "lect_01", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.1-runcell01\n", "\n", "import scipy.stats as stats\n", "\n", "sigma = 1\n", "print(stats.norm.cdf(sigma)-stats.norm.cdf(-sigma))\n", "\n", "def pval(iVal):\n", "    return stats.norm.cdf(iVal)-stats.norm.cdf(-iVal)\n", "\n", "def chi2Val(iGausSigma,iNDOF):\n", "    val=stats.chi2.ppf(pval(iGausSigma),iNDOF)\n", "    return val\n", "\n", "print(chi2Val(1,2))\n", "#print(chi2Val(1,1))\n", "#print(chi2Val(2,1))\n", "#print(chi2Val(3,1))\n", "#print(chi2Val(4,1))"]}, {"cell_type": "markdown", "id": "6b30ab91", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_7_1'></a>     \n", "\n", "| [Top](#section_7_0) | [Restart Section](#section_7_1) | [Next Section](#section_7_2) |\n"]}, {"cell_type": "markdown", "id": "c1b07b38", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-7.1.1</span>\n", "\n", "Compute the Chi-square value for 2$\\sigma$ and 2 degrees of freedom. Enter your answer as a number with precision 1e-2. \n"]}, {"cell_type": "code", "execution_count": null, "id": "a9a30c37", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L7.1.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n"]}, {"cell_type": "markdown", "id": "9fed2264", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_7_2'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L7.2 Minimizing on a Surface (1D Scan)</h2>  \n", "\n", "| [Top](#section_7_0) | [Previous Section](#section_7_1) | [Exercises](#exercises_7_2) | [Next Section](#section_7_3) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "5b99381c", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.2-slides\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L07/slides_L07_02.html', width=970, height=550)"]}, {"cell_type": "code", "execution_count": null, "id": "c88f8011", "metadata": {"scrolled": true, "tags": ["learner", "py", "lect_01", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.2-runcell01\n", "\n", "############ This is all code from a previous lesson\n", "import numpy as np\n", "import csv\n", "import math\n", "import lmfit\n", "import matplotlib.pyplot as plt\n", "from scipy import optimize as opt\n", "\n", "def rad(iTheta):\n", "    return iTheta/180. * math.pi\n", "\n", "def rad1(iTheta):\n", "    return iTheta/180. * math.pi-math.pi\n", "\n", "def load(label):\n", "    dec=np.array([])\n", "    ra=np.array([])\n", "    az=np.array([])\n", "    with open(label,'r') as csvfile:\n", "        plots = csv.reader(csvfile,delimiter=' ')\n", "        for pRow in plots:\n", "            if '#' in pRow[0] or pRow[0]=='':\n", "                continue\n", "            dec = np.append(dec,rad(float(pRow[2])))\n", "            ra  = np.append(ra,rad1(float(pRow[3])))\n", "            az  = np.append(az,rad(float(pRow[4])))\n", "    return dec,ra,az\n", "\n", "def prephist(iRA):\n", "    y0, bin_edges = np.histogram(iRA, bins=30)\n", "    x0 = 0.5*(bin_edges[1:] + bin_edges[:-1])\n", "    y0 = y0.astype('float')\n", "    return x0,y0,1./(y0**0.5)\n", "\n", "label8='data/L05/events_a8_1space.dat'\n", "\n", "dec,ra8,az=load(label8)\n", "xhist,yhist,xweights=prephist(ra8)\n", "\n", "\n", "########## Tlast fit code\n", "\n", "def fnew(x,a,b):\n", "    pVal=b*np.sin(x)\n", "    return a+pVal\n", "\n", "model  = lmfit.Model(fnew)\n", "p = model.make_params(a=1000,b=10)\n", "result = model.fit(data=yhist,x=xhist, params=p, weights=xweights)\n", "lmfit.report_fit(result)\n", "plt.figure()\n", "result.plot()\n", "plt.show()\n"]}, {"cell_type": "code", "execution_count": null, "id": "4b5fc9a6", "metadata": {"tags": ["learner", "py", "lect_02", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.2-runcell02\n", "\n", "#This is the new code\n", "def chi2(iX):\n", "    #Note that this function depends on xhist and yhist being defined already.\n", "    #This is bad practice in general, but we do it here for convenience.\n", "    #After all, this code isn't reused elsewhere.\n", "    #MAKE SURE YOU RUN prephist(ra8) to define yhist\n", "    assert len(iX) == 2\n", "    lTot=0\n", "    for val in range(len(yhist)):\n", "        xtest=fnew(xhist[val],iX[0],iX[1])\n", "        lTot += (1./(xtest+1e-5))*(yhist[val]-xtest)**2\n", "    return lTot\n", "\n", "#First we minimize\n", "x0 = np.array([1000,10]) # initial conditions\n", "ps = [x0]\n", "sol=opt.minimize(chi2, x0)\n", "print(sol)"]}, {"cell_type": "code", "execution_count": null, "id": "26616a72", "metadata": {"tags": ["learner", "py", "lect_02", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.2-runcell03\n", "\n", "#Scan near the minimum of each value\n", "x = np.linspace(sol.x[0]*0.99,sol.x[0]*1.01, 100)\n", "y = np.linspace(sol.x[1]*0.5,sol.x[1]*1.5, 100)\n", "\n", "#Now let's fix one parameter at the minimum, and profile the other\n", "plt.plot(x, chi2([x,sol.x[1]]),label='chi2');\n", "plt.axhline(sol.fun+1, c='red')\n", "plt.xlabel(\"a-value\")\n", "plt.ylabel(\"$\\chi^{2}$\")\n", "plt.show()\n", "\n", "#Now for the other parameter\n", "plt.plot(y, chi2([sol.x[0],y]),label='chi2');\n", "plt.axhline(sol.fun+1, c='red')\n", "plt.xlabel(\"b-value\")\n", "plt.ylabel(\"$\\chi^{2}$\")\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "b991443a", "metadata": {"tags": ["learner", "py", "lect_02", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.2-runcell04\n", "\n", "######## Now let's use a numerical solver to find the points at which a function crosses zero (root solver)\n", "def chi2minX(xval, delta_chi2=1):\n", "    val=chi2([xval,sol.x[1]])\n", "    minval=chi2(sol.x) + delta_chi2\n", "    return val-minval\n", "\n", "def chi2minY(yval, delta_chi2=1):\n", "    val=chi2([sol.x[0],yval])\n", "    minval=chi2(sol.x) + delta_chi2\n", "    return val-minval\n", "\n", "def chi2uncX(sol):\n", "    solX1=opt.root_scalar(chi2minX,bracket=[sol.x[0], sol.x[0]*1.02],method='brentq')\n", "    solX2=opt.root_scalar(chi2minX,bracket=[sol.x[0]*0.98, sol.x[0]],method='brentq')\n", "    print(\"a:\",sol.x[0],\"+/-\",abs(solX2.root-solX1.root)/2.)\n", "    print(\"Reminder the Poisson uncertainty would be:\",math.sqrt(sol.x[0]/40))\n", "    return solX1, solX2\n", "\n", "def chi2uncY(sol):\n", "    solY1=opt.root_scalar(chi2minY,bracket=[sol.x[1],    sol.x[1]*1.2],method='brentq')\n", "    solY2=opt.root_scalar(chi2minY,bracket=[sol.x[1]*0.8, sol.x[1]],method='brentq')\n", "    print(\"b:\",sol.x[1],\"+/-\",abs(solY2.root-solY1.root)/2.)\n", "    return solY1, solY2\n", "\n", "solX1, solX2 = chi2uncX(sol)\n", "solY1, solY2 = chi2uncY(sol)"]}, {"cell_type": "markdown", "id": "290142a4", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_7_2'></a>     \n", "\n", "| [Top](#section_7_0) | [Restart Section](#section_7_2) | [Next Section](#section_7_3) |\n"]}, {"cell_type": "markdown", "id": "db70b48a", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-7.2.1</span>\n", "\n", "What are the 2$\\sigma$ bounds (that is, 95.45% confidence interval values) of $a$ and $b$ in the fit above? Enter your answer as a list of numbers with precision 1 (the nearest whole number): `[a_lower, a_upper, b_lower, b_upper]`"]}, {"cell_type": "code", "execution_count": null, "id": "725bd600", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L7.2.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "#For a given parameter, assume everything else is determined, so we're\n", "# working with 1 degree of freedom.\n", "# First, we want to know the chi-square value at which 2 standard deivations (95.45% of the probability)\n", "# in the chi-square distribution is for values less extreme than that value.\n", "ndof=1\n", "pval=0.9545\n", "val = stats.chi2.ppf(pval,ndof)\n", "print(\"Delta Chi-square:\", val)\n", "\n", "def minXfunc(x):\n", "    return chi2minX(x, delta_chi2=val)\n", "\n", "def minYfunc(x):\n", "    return chi2minY(x, delta_chi2=val)\n", "\n", "def chi2unc(sol, sol_index, min_func, unc_prop_guess):\n", "     #insert code here\n", "    return valmin,valmax\n", "\n", "a1, a2 = chi2unc(sol, 0, minXfunc, 0.08)\n", "b1, b2 = chi2unc(sol, 1, minYfunc, 0.4)\n", "\n", "print(f\"a bounds: [{a1}, {a2}]\")\n", "print(f\"b bounds: [{b1}, {b2}]\")"]}, {"cell_type": "markdown", "id": "14c0d887", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_7_3'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L7.3 Minimizing on a Surface (2D Scan)</h2>  \n", "\n", "| [Top](#section_7_0) | [Previous Section](#section_7_2) | [Exercises](#exercises_7_3) | [Next Section](#section_7_4) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "71725fcc", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.3-slides\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L07/slides_L07_03.html', width=970, height=550)"]}, {"cell_type": "code", "execution_count": null, "id": "c3feb0e5", "metadata": {"tags": ["learner", "py", "lect_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.3-runcell01\n", "\n", "#define the 2D X and Y grid\n", "x = np.linspace(sol.x[0]*0.99,sol.x[0]*1.01, 100) #grid in x\n", "y = np.linspace(sol.x[1]*0.5,sol.x[1]*1.5, 100)#grid in y\n", "X, Y = np.meshgrid(x, y) #2d grid\n", "\n", "# For z coordinate, evaluate chi2 at each x,y point in the grid.\n", "# note understanding this one-liner itself isn't too important\n", "Z = np.array([chi2([x,y]) for (x,y) in zip(X.ravel(), Y.ravel())]).reshape(X.shape)\n", "\n", "#and plot\n", "def plotColorsAndContours(X,Y,Z):\n", "    fig, ax = plt.subplots(1, 1)\n", "    c = ax.pcolor(X,Y,Z,cmap='RdBu')\n", "    cb=fig.colorbar(c, ax=ax)\n", "    plt.xlabel(\"a\")\n", "    plt.ylabel(\"b\")\n", "    cb.set_label(\"$\\chi^{2}$\")\n", "    #Now let's plot the contours of Delta chi^2\n", "    levels = [0.1,1,2.3,4,6.18,9, 16, 25, 36, 49, 64, 81, 100]\n", "    for i0 in range(len(levels)):\n", "        levels[i0] = levels[i0]+sol.fun\n", "    c = plt.contour(X, Y, Z, levels,colors=['red', 'blue', 'yellow','green'])\n", "    #plt.show()\n", "    \n", "plotColorsAndContours(X,Y,Z)"]}, {"cell_type": "code", "execution_count": null, "id": "a3d5ff96", "metadata": {"tags": ["learner", "py", "lect_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.3-runcell02\n", "\n", "#Let's plot the uncertainties  from hess_inv\n", "print(np.sqrt(2*sol.hess_inv))\n", "#the diagonals are approximately the errors\n", "\n", "#Make a the expression in the above equation x and x0 are 2 vectors\n", "def quadratic2D(x,x0,sigma0):\n", "    lVals=x-x0\n", "    lVals=(lVals**2)/(sigma0)/sigma0\n", "    return np.sum(lVals)\n", "\n", "plotColorsAndContours(X,Y,Z)\n", "\n", "#Now plot the ellipse in 3D\n", "def plotEllipse(sigx,sigy):\n", "    levels = [0.1,1,2.3,4,6.18,9, 16, 25, 36, 49, 64, 81, 100]\n", "    ZQ = np.array([quadratic2D([x,y],sol.x,[sigx,sigy]) for (x,y) in zip(X.ravel(), Y.ravel())]).reshape(X.shape)\n", "    c = plt.contour(X, Y, ZQ, levels,colors=['red', 'blue', 'yellow','green'],linestyles='dashed')\n", "\n", "sigx=(solX2.root-solX1.root)/2.\n", "sigy=(solY2.root-solY1.root)/2.\n", "plotEllipse(sigx,sigy)\n", "plt.show()"]}, {"cell_type": "markdown", "id": "cb948a8a", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_7_3'></a>     \n", "\n", "| [Top](#section_7_0) | [Restart Section](#section_7_3) | [Next Section](#section_7_4) |\n"]}, {"cell_type": "markdown", "id": "52737170", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-7.3.1</span>\n", "\n", "When we allow 2 parameters to vary, for a given confidence level (e.g. 1$\\sigma$) we end up with a confidence ellipse containing parameter values outside the confidence intervals on the individual parameters alone. Compute the 1$\\sigma$ (68.27%) confidence interval bounds for the parameter $a$, based on this ellipse from floating both $a$ and $b$.\n", "\n", "Enter your answer as a list of number with precision 1 (nearest whole number): `[a_lower, a_upper]`\n", "\n", "Why do we typically quote the uncertainty from the 1D variation?"]}, {"cell_type": "code", "execution_count": null, "id": "39befb5a", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L7.3.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "# We can follow the same procedure for finding the confidence interval based on 1D\n", "# but for 1-sigma we now use 2.3 \n", "# Use the code from the solution to Ex. 7.2.1,\n", "ndof=#insert value here\n", "pval=0.6827#1 sigma p-value\n", "val = stats.chi2.ppf(pval,ndof) # 2 DoF this time!\n", "print(\"Delta Chi-square:\", val)\n", "\n", "def minfunc(x):\n", "    return chi2minX(x, delta_chi2=val)\n", "\n", "a1, a2 = #YOUR CODE HERE\n", "\n", "print(f\"a bounds: [{a1}, {a2}]\")"]}, {"cell_type": "markdown", "id": "510af4b6", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_7_4'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L7.4 Correlations Between Fit Parameters: Part 1</h2>  \n", "\n", "| [Top](#section_7_0) | [Previous Section](#section_7_3) | [Exercises](#exercises_7_4) | [Next Section](#section_7_5) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "e64965cf", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.4-slides\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L07/slides_L07_04.html', width=970, height=550)"]}, {"cell_type": "code", "execution_count": null, "id": "c89f20c6", "metadata": {"tags": ["learner", "py", "lect_04", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.4-runcell01\n", "\n", "def fnew(x,a,b):\n", "    pVal=b*(1-x)\n", "    return a*x+pVal\n", "\n", "model  = lmfit.Model(fnew)\n", "p = model.make_params(a=1000,b=1000)\n", "\n", "result = model.fit(data=yhist,x=xhist, params=p, weights=xweights)\n", "lmfit.report_fit(result)\n", "plt.figure()\n", "result.plot()\n", "plt.show()\n", "\n", "x0 = np.array([1000,1000])\n", "ps = [x0]\n", "sol=opt.minimize(chi2, x0)\n", "print(sol)\n", "\n", "x = np.linspace(sol.x[0]*0.99,sol.x[0]*1.01, 100)\n", "y = np.linspace(sol.x[1]*0.99,sol.x[1]*1.01, 100)\n", "X, Y = np.meshgrid(x, y)\n", "Z = np.array([chi2([x,y]) for (x,y) in zip(X.ravel(), Y.ravel())]).reshape(X.shape)\n", "plotColorsAndContours(X,Y,Z)\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "24bc5b47", "metadata": {"tags": ["learner", "lect_04", "learner_chopped", "py"]}, "outputs": [], "source": ["#>>>RUN: L7.4-runcell02\n", "\n", "plotColorsAndContours(X,Y,Z)\n", "solX1, solX2 = chi2uncX(sol)\n", "solY1, solY2 = chi2uncY(sol)\n", "sigx=(solX2.root-solX1.root)/2.\n", "sigy=(solY2.root-solY1.root)/2.\n", "plotEllipse(sigx,sigy)\n", "plt.show()\n"]}, {"cell_type": "markdown", "id": "aa61b45e", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_7_4'></a>     \n", "\n", "| [Top](#section_7_0) | [Restart Section](#section_7_4) | [Next Section](#section_7_5) |\n"]}, {"cell_type": "markdown", "id": "600b5811", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-7.4.1</span>\n", "\n", "Run the previous fit with just a regular linear fit given by $f(x) = ax + b$. How does the $\\chi^{2}$ change? How do the correlations between the variables change? Choose the best option from the following:\n", "\n", "- $\\chi^{2}$ increases and the correlations increase\n", "- $\\chi^{2}$ decreases and the correlations increase\n", "- $\\chi^{2}$ stays approximately the same and the correlations increase\n", "- $\\chi^{2}$ increases and the correlations decrease\n", "- $\\chi^{2}$ decreases and the correlations decrease\n", "- $\\chi^{2}$ stays approximately the same and the correlations decrease\n", "- $\\chi^{2}$ increases and the correlations stay approximately the same\n", "- $\\chi^{2}$ decreases and the correlations stay approximately the same\n", "- $\\chi^{2}$ stays approximately the same and the correlations stay approximately the same"]}, {"cell_type": "code", "execution_count": null, "id": "ff980996", "metadata": {"tags": ["draft", "py"]}, "outputs": [], "source": ["#>>>EXERCISE: L7.4.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n"]}, {"cell_type": "markdown", "id": "1118b3c4", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_7_5'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L7.5 Correlations Between Fit Parameters: Part 2</h2>     \n", "\n", "| [Top](#section_7_0) | [Previous Section](#section_7_4) | [Exercises](#exercises_7_5) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "2c09be2f", "metadata": {"tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.5-runcell01\n", "\n", "def fnew(x,a,b):\n", "    pVal=b*(1-x)\n", "    return a*x+pVal\n", "\n", "model  = lmfit.Model(fnew)\n", "p = model.make_params(a=1000,b=1000)\n", "\n", "result = model.fit(data=yhist,x=xhist, params=p, weights=xweights)\n", "lmfit.report_fit(result)\n", "plt.figure()\n", "result.plot()\n", "plt.show()\n", "\n", "x0 = np.array([1000,1000])\n", "ps = [x0]\n", "sol=opt.minimize(chi2, x0)\n", "print(sol)\n", "\n", "x = np.linspace(sol.x[0]*0.99,sol.x[0]*1.01, 100)\n", "y = np.linspace(sol.x[1]*0.99,sol.x[1]*1.01, 100)\n", "X, Y = np.meshgrid(x, y)\n", "Z = np.array([chi2([x,y]) for (x,y) in zip(X.ravel(), Y.ravel())]).reshape(X.shape)\n", "plotColorsAndContours(X,Y,Z)\n", "plt.show()\n"]}, {"cell_type": "code", "execution_count": null, "id": "12807150", "metadata": {"tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.5-runcell02\n", "\n", "print(np.sqrt(2*sol.hess_inv))\n", "#The diagonals are the uncertainty lmfit quotes\n", "\n", "#Really the best way to do this is to get the eigen values using an linear algebra problem\n", "import numpy.linalg as la\n", "w, v=la.eig(2*sol.hess_inv)\n", "print(\"values\",w,\"vectors\",v)\n", "\n", "#Now let's plot the eigenvectors\n", "from matplotlib.patches import Ellipse\n", "def get_cov_ellipse(cov, centre, nstd, **kwargs):\n", "    \"\"\"\n", "    Return a matplotlib Ellipse patch representing the covariance matrix\n", "    cov centred at centre and scaled by the factor nstd.\n", "\n", "    \"\"\"\n", "    # Find and sort eigenvalues and eigenvectors into descending order\n", "    eigvals, eigvecs = np.linalg.eigh(cov)\n", "    order = eigvals.argsort()[::-1]\n", "    eigvals, eigvecs = eigvals[order], eigvecs[:, order]\n", "\n", "    # The anti-clockwise angle to rotate our ellipse by \n", "    vx, vy = eigvecs[:,0][0], eigvecs[:,0][1]\n", "    theta = np.arctan2(vy, vx)\n", "\n", "    # Width and height of ellipse to draw\n", "    width, height = 2 * nstd * np.sqrt(eigvals)\n", "    return Ellipse(xy=centre, width=width, height=height,angle=np.degrees(theta), **kwargs)\n", "\n", "err_ellipse=get_cov_ellipse(2*sol.hess_inv,sol.x,1)\n", "fig, ax = plt.subplots(1, 1)\n", "c = ax.pcolor(X,Y,Z,cmap='RdBu')\n", "fig.colorbar(c, ax=ax)\n", "levels = [0.1,1,2.3,4,9, 16, 25, 36, 49, 64, 81, 100]\n", "for i0 in range(len(levels)):\n", "    levels[i0] = levels[i0]+sol.fun\n", "c = plt.contour(X, Y, Z, levels,colors=['red', 'blue', 'yellow','green'])\n", "ax.add_artist(err_ellipse)\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "6719aad5", "metadata": {"tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.5-runcell03\n", "\n", "#Now let's get the correlation C(a,b) (see below)\n", "w, v=np.linalg.eig(2*sol.hess_inv)\n", "print(\"c(a,b)\",v[0,1]/v[0,0])\n", "print(\"A deceptively wrong way to get correlation: since its not normalized\",sol.hess_inv[0,1]/sol.hess_inv[0,0])"]}, {"cell_type": "code", "execution_count": null, "id": "6033110d", "metadata": {"tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.5-runcell04\n", "\n", "import lmfit\n", "\n", "def fnew(x,a,b):\n", "    pVal=b*(1-x)\n", "    return a*x+pVal\n", "\n", "#Randomly sample points in the above range\n", "def maketoy(iy):\n", "    toy=np.array([])\n", "    #go through the y-values and Poisson fluctuate\n", "    for i0 in range(len(iy)):\n", "        pVal = np.random.normal (iy[i0],np.sqrt([iy[i0]]))\n", "        toy = np.append(toy,float(pVal))\n", "    return toy\n", "\n", "def fittoy(ibin,iy):\n", "    #generate toy\n", "    toy=maketoy(iy)\n", "    #now fit\n", "    model  = lmfit.Model(fnew)\n", "    p = model.make_params(a=1000,b=10)\n", "    xweights=np.array([])\n", "    #setup poison weight\n", "    for i0 in range(len(toy)):\n", "        xweights = np.append(xweights,1./math.sqrt(toy[i0]))\n", "    result = model.fit(data=toy,x=ibin, params=p, weights=xweights)\n", "    return result.params[\"a\"].value,result.params[\"b\"].value\n", "\n", "ntoys=1000\n", "lAs=np.array([])\n", "lBs=np.array([])\n", "for i0 in range(ntoys):\n", "    pA,pB=fittoy(xhist,yhist)\n", "    lAs = np.append(lAs,pA)\n", "    lBs = np.append(lBs,pB)\n", "\n", "err_ellipse=get_cov_ellipse(2*sol.hess_inv,sol.x, 1)\n", "fig, ax = plt.subplots(1, 1)\n", "c = ax.pcolor(X,Y,Z,cmap='RdBu')\n", "fig.colorbar(c, ax=ax)\n", "c = plt.contour(X, Y, Z, levels,colors=['red', 'blue', 'yellow','green'])\n", "ax.add_artist(err_ellipse)\n", "plt.plot(lAs,lBs,c='black',marker='.',linestyle = 'None')\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "a5ae9db2", "metadata": {"tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.5-runcell05\n", "\n", "#Now let's run a. linear regression\n", "def variance(isamples):\n", "    mean=isamples.mean()\n", "    n=len(isamples)\n", "    tot=0\n", "    for pVal in isamples:\n", "        tot+=(pVal-mean)**2\n", "    return tot/n\n", "\n", "def covariance(ixs,iys):\n", "    meanx=ixs.mean()\n", "    meany=iys.mean()\n", "    n=len(ixs)\n", "    tot=0\n", "    for i0 in range(len(ixs)):\n", "        tot+=(ixs[i0]-meanx)*(iys[i0]-meany)\n", "    return tot/n\n", "\n", "print(\"A:\",lAs.mean(),\"+/-\",lAs.std())\n", "print(\"B:\",lBs.mean(),\"+/-\",lBs.std())\n", "print(\"Cov:\",covariance(lAs,lBs),\"A Variance:\",variance(lAs),\"B Variance:\",variance(lBs))\n", "print(\"Check with Hessian:\",2*sol.hess_inv)\n", "print(\"Cor:\",covariance(lAs,lBs)/math.sqrt(variance(lAs)*variance(lBs)),\"A Variance:\",1.,\"B Variance:\",1.)"]}, {"cell_type": "markdown", "id": "a4251144", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_7_5'></a>   \n", "\n", "| [Top](#section_7_0) | [Restart Section](#section_7_5) |\n"]}, {"cell_type": "markdown", "id": "0b0b7275", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-7.5.1</span>\n", "\n", "Repeat the previous analysis using the uncorrelated fit, $f(x) = a x + b$. Specifically, plot the ellipse and compute the uncertainties in $a$ and $b$? Do they correspond with lmfit? \n"]}, {"cell_type": "code", "execution_count": null, "id": "9a896c73", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L7.5.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n"]}], "metadata": {"celltoolbar": "Tags", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}