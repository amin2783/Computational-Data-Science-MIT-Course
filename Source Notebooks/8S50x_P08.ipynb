{"cells": [{"cell_type": "markdown", "id": "02dd2f9a", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<hr style=\"height: 1px;\">\n", "<i>This notebook was authored by the 8.S50x Course Team, Copyright 2022 MIT All Rights Reserved.</i>\n", "<hr style=\"height: 1px;\">\n", "<br>\n", "\n", "<h1>Guided Problem Set 8: Covariance and Correlation</h1>\n"]}, {"cell_type": "markdown", "id": "7d8b809e", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_8_0'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">P8.0 Overview</h2>\n"]}, {"cell_type": "markdown", "id": "fc477952", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<h3>Navigation</h3>\n", "\n", "<table style=\"width:100%\">\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_8_1\">P8.1 Multivariable Continuous Distributions and Covariance</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#problems_8_1\">P8.1 Problems</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_8_2\">P8.2 Worked Covariance Examples and Problems</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#problems_8_2\">P8.2 Problems</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_8_3\">P8.3 Correlation Coefficient</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#problems_8_3\">P8.3 Problems</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_8_4\">P8.4 Covariance Matrix and Computational Examples</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#problems_8_4\">P8.4 Problems</a></td>\n", "    </tr>\n", "</table>\n", "\n"]}, {"cell_type": "markdown", "id": "178cf1d4", "metadata": {"tags": ["learner", "catsoop_00", "md"]}, "source": ["<h3>Learning Objectives</h3>\n", "\n", "In this recitation we will explore the following objectives:\n", "\n", "- Understand the derivation and conceptual basis for covariance and how to calculate it\n", "- How to estimate covariance values from given distributions of data\n", "- The limitations of the covariance value and subsequently the derivation and use of the correlation coefficient\n", "- The covariance matrix and how to use it to calculate covariance and correlation coefficients for multiple variables"]}, {"cell_type": "markdown", "id": "7239127a", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<h3>Importing Data (Colab Only)</h3>\n", "\n", "If you are in a Google Colab environment, run the cell below to import the data for this notebook. Otherwise, if you have downloaded the course repository, you do not have to run the cell below.\n", "\n", "See the source and attribution information below:\n", "\n", ">data: data/P07/DatasaurusDozen.tsv <br>\n", ">source: https://damassets.autodesk.net/content/dam/autodesk/research/publications-assets/pdf/same-stats-different-graphs.pdf, http://www.thefunctionalart.com/2016/08/download-datasaurus-never-trust-summary.html <br>\n", ">attribution data:  Alberto Cairo <br>\n", ">attribution paper: Autodesk, Justin Matejka, George Fitzmaurice, CHI '17: Proceedings of the 2017 CHI Conference on Human Factors in Computing SystemsMay 2017 Pages 1290\u20131294https://doi.org/10.1145/3025453.3025912"]}, {"cell_type": "code", "execution_count": null, "id": "47d8494d", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: P8.0-runcell01\n", "\n", "!git init\n", "!git remote add -f origin https://github.com/mitx-8s50/nb_LEARNER/\n", "!git config core.sparseCheckout true\n", "!echo 'P07' >> .git/info/sparse-checkout\n", "!git pull origin main"]}, {"cell_type": "markdown", "id": "5cf300ff", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Importing Libraries</h3>\n", "\n", "Before beginning, run the cell below to import the relevant libraries for this notebook."]}, {"cell_type": "code", "execution_count": null, "id": "06bdc10f", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: P8.0-runcell02\n", "\n", "import numpy as np               #https://numpy.org/doc/stable/\n", "import matplotlib.pyplot as plt  #https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.html\n", "\n", "import pandas as pd              #https://pandas.pydata.org/docs/user_guide/index.html"]}, {"cell_type": "markdown", "id": "2305bb60", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Setting Default Figure Parameters</h3>\n", "\n", "The following code cell sets default values for figure parameters."]}, {"cell_type": "code", "execution_count": null, "id": "f957ae03", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: P8.0-runcell03\n", "\n", "#set plot resolution\n", "%config InlineBackend.figure_format = 'retina'\n", "\n", "#set default figure parameters\n", "plt.rcParams['figure.figsize'] = (9,6)\n", "\n", "medium_size = 12\n", "large_size = 15\n", "\n", "plt.rc('font', size=medium_size)          # default text sizes\n", "plt.rc('xtick', labelsize=medium_size)    # xtick labels\n", "plt.rc('ytick', labelsize=medium_size)    # ytick labels\n", "plt.rc('legend', fontsize=medium_size)    # legend\n", "plt.rc('axes', titlesize=large_size)      # axes title\n", "plt.rc('axes', labelsize=large_size)      # x and y labels\n", "plt.rc('figure', titlesize=large_size)    # figure title\n"]}, {"cell_type": "markdown", "id": "b7a7f5b7", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_8_1'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">P8.1 Multivariable Continuous Distributions and Covariance</h2>    \n", "\n", "| [Top](#section_8_0) | [Previous Section](#section_8_0) | [Problems](#problems_8_1) | [Next Section](#section_8_2) |\n"]}, {"cell_type": "markdown", "id": "267f5cf7", "metadata": {"tags": ["learner", "md", "catsoop_01"]}, "source": ["<h3>Overview</h3>\n", "\n", "We can define a joint distribution function for two variables $x,y$ as $F(x,y)=P\\{(x' \\lt x)\\cap (y' \\lt y)\\}$ that has the two properties:\n", "\n", "- $F(\\infty,\\infty)=1$\n", "- $F(-\\infty,y)=F(x,-\\infty)=0$\n", "- $F$ is monotonically increasing.\n", "\n", "This is useful but overshadowed by the much more useful probability density which we can define as $f(x,y)=\\frac{\\partial^2 F}{\\partial x \\partial y}$ which, by convention, is usually normalized, i.e. $\\iint \\text{d}x\\text{d}y f(x,y)=1$. Given this multivariable distribution we can calculate its moments:\n", "\n", "\n", "Means:\n", "\n", "- $\\mu_x = \\langle x \\rangle$\n", "\n", "- $\\mu_y = \\langle y \\rangle$\n", "\n", "Variances:\n", "\n", "- $\\text{var}(x)=\\sigma_x^2 = \\langle (x-\\mu_x)^2\\rangle$\n", "- $\\text{var}(y)=\\sigma_y^2 = \\langle (y-\\mu_y)^2\\rangle$\n", "- $\\text{var}(x,y)=\\sigma_{x,y}=\\langle(x-\\mu_x)(y-\\mu_y)\\rangle=\\langle{xy}\\rangle-\\langle{x}\\rangle\\langle{y}\\rangle$\n", "\n", "The first few terms are nothing new that we haven't seen from single variable distributions, but here we introduce the new 'mixed variance' term combining the variance of $x$ and $y$. This value is called the $\\mathrm{covariance}$ and is often written as $\\mathrm{cov}(x,y)$."]}, {"cell_type": "markdown", "id": "fe08ba86", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='problems_8_1'></a>     \n", "\n", "| [Top](#section_8_0) | [Restart Section](#section_8_1) | [Next Section](#section_8_2) |\n"]}, {"cell_type": "markdown", "id": "a62020af", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Problem 8.1.1</span>\n", "\n", "Write a python function that takes in an array of single variable data and outputs its variance. Do not use any `numpy` functions, instead consider `sum()` and `len()` to perform operations on arrays.\n"]}, {"cell_type": "code", "execution_count": null, "id": "021cbe77", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>PROBLEM: P8.1.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "\n", "def get_variance(x):\n", "\n", "    ################################\n", "    ####### INSERT CODE HERE #######\n", "    ################################\n", "\n", "    return x_variance\n", "\n", "\n", "#TEST\n", "sigma=2\n", "x = np.random.normal(0,sigma,100000)\n", "\n", "print(\"calculated:\",get_variance(x),\"expected:\",sigma**2)\n", "\n", "#Alternatively\n", "print(\"Quick Variance:\",x.var())"]}, {"cell_type": "markdown", "id": "f6b54aa9", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Problem 8.1.2</span>\n", "\n", "Write a python function that takes in 2 arrays of data and outputs their covariance. Specifically, use the definition $\\sigma_{x,y}=\\langle{xy}\\rangle-\\langle{x}\\rangle\\langle{y}\\rangle$. Again, do not use any `numpy` functions in your solution.\n", "\n", "Is your result different from implementing the definition $\\sigma_{x,y}=\\langle(x-\\mu_x)(y-\\mu_y)\\rangle$? \n", "\n", "What about the `numpy` function `np.cov[x,y]`? For more information about the `numpy` comparison, see <a href=\"https://stackoverflow.com/questions/48881499/why-is-numpys-covariance-slightly-different-to-manually-computing\" target=\"_blank\">here</a> and <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.cov.html\" target=\"_blank\">here</a>.\n", "\n"]}, {"cell_type": "code", "execution_count": null, "id": "e43b7ea3", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>PROBLEM: P8.1.2\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def get_covariance(x,y):\n", "\n", "    ################################\n", "    ####### INSERT CODE HERE #######\n", "    ################################\n", "    \n", "    return xy_variance\n", "\n", "print(\"covariance version 1:\",get_covariance(x,y))"]}, {"cell_type": "markdown", "id": "d6c4adb2", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_8_2'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">P8.2 Worked Covariance Examples and Problems</h2>    \n", "\n", "| [Top](#section_8_0) | [Previous Section](#section_8_1) | [Problems](#problems_8_2) | [Next Section](#section_8_3) |\n"]}, {"cell_type": "markdown", "id": "b1264cd6", "metadata": {"tags": ["learner", "catsoop_02", "md"]}, "source": ["<h3>Examples of Covariance</h3>\n", "\n", "We can take a look at some linear distributions to get a better intuition for what the covariance represents:\n", "\n", "<p align=\"center\">\n", "<img alt=\"different scatter plots 1\" src=\"https://raw.githubusercontent.com/mitx-8s50/images/main/P07/Figure_1.png\" width=\"800\"/>\n", "</p>\n", "\n", "We can see that Plots 1 and 3 have very clear linear relationships between $x$ and $y$ resulting in large covariance values. We also notice that the sign of the covariance matches the sign of the slope of the linear relationship. The way cases like these are typically described is that the two variables are \"highly correlated\". Note that this is exact the opposite of what we earlier called \"independent variables\". \n", "\n", "By contrast, Plot 2 has a covariance value that is approximately zero since there isn't any distinguishable relationship between $x$ and $y$ for the data given. This is the case for independent variables, as these two variables are described as \"uncorrelated\".\n", "\n", "So, the covariance is a quantitative measure of how close two variables are to being independent.<br>"]}, {"cell_type": "markdown", "id": "76c5740b", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='problems_8_2'></a>     \n", "\n", "| [Top](#section_8_0) | [Restart Section](#section_8_2) | [Next Section](#section_8_3) |\n"]}, {"cell_type": "markdown", "id": "7aae0c1b", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Problem 8.2.1</span>\n", "\n", "Using this, let's try to rank the covariance values for the following distributions:\n", "\n", "<p align=\"center\">\n", "<img alt=\"different scatter plots 2\" src=\"https://raw.githubusercontent.com/mitx-8s50/images/main/P07/Figure_2.png\" width=\"850\"/>\n", "</p>\n", "\n", "Which of the following is the correct ordering of the value of $\\mathrm{cov}(x,y)$ in the plots above (from lowest to highest)? Consider the covariances as signed quantities, where negative values are considered lower than positive ones.\n", "\n", "\n", "- $A \\lt B \\lt C \\lt D$\n", "\n", "- $B \\lt A \\lt D \\lt C$\n", "\n", "- $D \\lt A \\lt B \\lt C$\n", "\n", "- $D \\lt B \\lt C \\lt A$\n", "\n", "- $D \\lt C \\lt B \\lt A$\n", "\n", "- $A \\approx B \\lt C \\approx D$\n", "\n", "- $C \\approx D \\lt A \\approx B$"]}, {"cell_type": "markdown", "id": "92f7be12", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": [">#### Follow-up 8.2.1a (ungraded)\n", ">\n", ">As you have seen, the ranking of covariances from lowest to highest is not necessarily the same as the ranking of the correlation from weakest to strongest. Instead, the ranking from weakest to strongest is based on the absolute value of the covariance. However, absolute values often prove difficult in some analyses due to their lack of differentiability around 0. To address this, the square of the covariance is typically used.\n", ">\n", ">Rank the same 4 plots in terms of the square of their covariances."]}, {"cell_type": "markdown", "id": "bd926ddc", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_8_3'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">P8.3 Correlation Coefficient</h2>    \n", "\n", "| [Top](#section_8_0) | [Previous Section](#section_8_2) | [Problems](#problems_8_3) | [Next Section](#section_8_4) |\n"]}, {"cell_type": "markdown", "id": "66da5748", "metadata": {"tags": ["learner", "catsoop_03", "md"]}, "source": ["<h3>Overview</h3>\n", "\n", "While covariance is an important quantity in data analysis, it's difficult to determine the degree of correlation or to make comparisons between different sets of data using only the covariance. The most important reason why using covariance alone is problematic is that its value is not bounded.  As the range of $x$ and/or $y$ expands, $\\sigma_{x,y}$ increases without limitation as demonstrated in the code below. The second case, for which the two variables are almost uncorrelated, has a covariance which is almost a factor of 4 higher simply because it has a broader extent."]}, {"cell_type": "code", "execution_count": null, "id": "518f14b8", "metadata": {"tags": ["learner", "py", "learner_chopped", "catsoop_03"]}, "outputs": [], "source": ["#>>>RUN: P8.3-runcell01\n", "\n", "plt.rcParams['figure.figsize'] = (6,4)\n", "\n", "np.random.seed(90)\n", "\n", "sigma1=1\n", "x1 = np.random.normal(0,sigma1,10000)\n", "\n", "sigma2=0.01\n", "np.random.seed(908)\n", "shift1 = np.random.normal(0,sigma2,10000)\n", "y1 = x1 + shift1\n", "\n", "\n", "plt.scatter(x1,y1)\n", "plt.title(\"High correlation, smaller extent\")\n", "print(\"Two variables strongly correlated and with smaller extent: cov(x,y) = \",np.cov([x1,y1],bias=True)[0,1])\n", "plt.show()\n", "\n", "sigma3=5.5\n", "np.random.seed(91)\n", "x2 = np.random.normal(0,sigma3,10000)\n", "\n", "sigma4=40\n", "np.random.seed(909)\n", "shift2 = np.random.normal(0,sigma4,10000)\n", "y2 = 0.13*(x2 + shift2)\n", "\n", "plt.scatter(x2,y2)\n", "plt.title(\"Low correlation, larger extent\")\n", "print(\"Two variables weakly correlated and with larger extent: cov(x,y) = \",np.cov([x2,y2],bias=True)[0,1])\n", "plt.show()\n", "\n", "#reset default figure size\n", "plt.rcParams['figure.figsize'] = (9,6)"]}, {"cell_type": "markdown", "id": "5fd31a00", "metadata": {"tags": ["learner", "catsoop_03", "md"]}, "source": ["In order to address this deficiency in $\\sigma_{x,y}$, we introduce the ***correlation coefficient*** which is defined as follows:\n", "\n", "$$\\rho_{x,y}\\equiv \\sigma_{x,y}/\\sigma_x\\sigma_y$$\n", "\n", "Note that this quantity also has the advantage that it is dimensionless, so one could even compare datasets which measure different observables.\n", "\n", "By the Cauchy-Schwarz inequality and the 'geometric' relationship between $\\sigma_{x,y}$ and $\\sigma_x,\\sigma_y$, we have that $|\\rho_{x,y}|\\leq 1$. This $\\rho_{x,y}$ is known as Pearson's correlation coefficient and measures the degree to which the two variables are correlated.\n", "\n", "Similarly to the covariance values, $\\rho$ will take both positive and negative values depending on the sign of the slope, with $\\rho=\\pm 1$ indicating a perfect correlation between $x$ and $y$ and $\\rho=0$ occurring for independent variables which have no correlation at all. We can see this with some examples:\n", "\n", "<p align=\"center\">\n", "<img alt=\"different scatter plots 3\" src=\"https://raw.githubusercontent.com/mitx-8s50/images/main/P07/Figure_3.png\" width=\"900\"/>\n", "</p>\n", "\n", "Note, in particular, the three examples with $\\rho\\approx +0.8$ and $\\rho \\approx -0.7$ to $-0.8$. These clearly demonstrate that the correlation coefficient measures only how close to linear the relationship is, and is not affected by the slope of that linear dependence. "]}, {"cell_type": "markdown", "id": "083c4c82", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='problems_8_3'></a>     \n", "\n", "| [Top](#section_8_0) | [Restart Section](#section_8_3) | [Next Section](#section_8_4) |\n"]}, {"cell_type": "markdown", "id": "a6517d93", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Problem 8.3.1</span>\n", "\n", "Write a python function which takes in two sets of data and calculates the correlation coefficient between them. You may use previous functions you've calculated, `get_variance(x)` and `get_covariance(x,y)`, but do not use any other outside functions. Do not use `numpy`."]}, {"cell_type": "code", "execution_count": null, "id": "b7e7516d", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>PROBLEM: P8.3.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "np.random.seed(859303)\n", "\n", "x = np.linspace(0, 10, 100)\n", "y = [xi + np.random.normal(0,2,1)[0] for xi in x]\n", "\n", "plt.scatter(x,y)\n", "\n", "def get_correlation_coeff(x,y):\n", "    # INSERT CODE HERE\n", "    return\n", "\n", "\n", "print(\"correlation coefficient version 1:\",get_correlation_coeff(x,y))"]}, {"cell_type": "markdown", "id": "a8ddfe2d", "metadata": {"tags": ["learner", "md", "catsoop_03"]}, "source": ["<h3>How Data Uncertainties Affect the Correlation Coefficient</h3>\n", "\n", "One interesting question is how uncertainty in the dependent variable impacts the correlation coefficient. We can investigate this quantitatively by plotting the correlation coefficient between two quantities versus the standard deviation of the dependent variable. \n", "\n", "This can be done in many ways, but here are some suggestions for how to proceed:\n", "\n", "- Begin by creating a `np.linspace` for your $x$ variable, containing 1000 evenly spaced points from 0 to 10.\n", "\n", "- Then, create a `np.linspace` of standard deviations `std`, ranging from 0 to 150 and containing 150 points. In the next step, we will sample from Gaussians with a mean of zero and standard deviations given by the values in this range. These random \"uncertainties\" will be used to \"smear out\" the $y$ values found using the exact functional form. \n", "\n", "- Now, for the full array of $x$ values and each individual `std` value in your array, calculate the correlation coefficient between the values of $x$ and the smeared values $y(std)$. Each element of the array $y(std)$ should be related to $x$ by the following: $y_{i} = x_{i} + unc_{i}$, where $unc_{i}$ is a random uncertainty drawn from a normal distribution with a mean of 0 and a standard deviation of `std`.\n", "\n", "- Iterate the previous step over the full range of standard deviations and plot this as `(std_dev, correlation_coefficient_given_std_dev)`."]}, {"cell_type": "markdown", "id": "5405e846", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Problem 8.3.2</span>\n", "\n", "Complete the task outlined above to make a plot of `correlation_coefficient_given_std_dev` vs. `std_dev`. You may consider using the starting code below (and you may use `numpy`.) \n", "\n", "What is the general shape of the graph?\n", "\n", "- Steadily increasing\n", "\n", "- Steadily decreasing\n", "\n", "- Initially quickly decreasing, then flattening out\n", "\n", "- Initially quickly increasing, then flattening out\n", "\n", "- Flat at the beginning, then quickly increasing\n", "\n", "- Flat at the beginning, then quickly decreasing\n"]}, {"cell_type": "code", "execution_count": null, "id": "f56d38b5", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>PROBLEM: P8.3.2\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "np.random.seed(859303) \n", "\n", "std_devs = #YOUR CODE HERE\n", "x = #YOUR CODE HERE\n", "    \n", "correlation_coefficient_given_std_dev = []\n", "for std_dev in std_devs:\n", "    #YOUR CODE HERE\n", "    \n", "plt.plot(std_devs, correlation_coefficient_given_std_dev)\n", "plt.xlabel(\"Y  standard deviation\")\n", "plt.ylabel(\"Correlation coefficient\")\n", "plt.axhline(0,0,150)\n", "plt.show()"]}, {"cell_type": "markdown", "id": "a9cabb5b", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": [">#### Follow-up 8.3.2a (ungraded)\n", "> \n", ">The plot for Problem 8.3.2 shows that value of the correlation coefficient starts to fluctuate up and down quite a bit as the uncertainty in $y$ (Y standard deviation) gets larger. Why is this? How could you modify the code to make the correlation coefficient change more smoothly?"]}, {"cell_type": "markdown", "id": "d5b5bf1c", "metadata": {"tags": ["learner", "md", "catsoop_03"]}, "source": ["<h3> Correlation Coefficients for Non-linear Functional Forms</h3>\n", "\n", "In all of the examples shown above, $y$ fluctuates around a linear dependence on $x$ with a slope of unity, in other words $y=x+unc$ where $unc$ is some fluctuation or experimental uncertainty which causes $y$ to deviate from a perfect linear dependence on $x$. Small and large spreads of $unc$ correspond to large and small values of the correlation coefficient, respectively. \n", "\n", "Alternatively, if $y(x)$ has a different (i.e. non-linear) functional form, it is still true that large values of $\\sigma_{x,y}$ will be found if there are small fluctuations around the exact functional form $y=f(x)$. However, the limit of $\\sigma_{x,y}=1$ for a perfect correlation (i.e. $y=f(x)+unc$ where $unc \\approx 0$) no longer holds.\n", "\n", "This is demonstrated by the code shown below which calculates $\\sigma_{x,y}$ for three different functions. Try running with different values of `ysigma` (including a very small number) to see how the plots and the correlation coefficients change.\n", "\n", "We'll explore the relationship between the fluctuations and the correlation coefficients in more detail in Problem 8.3.3."]}, {"cell_type": "code", "execution_count": null, "id": "f3fa4bf1", "metadata": {"tags": ["learner", "py", "learner_chopped", "catsoop_03"]}, "outputs": [], "source": ["#>>>RUN: P8.3-runcell02\n", "\n", "plt.rcParams['figure.figsize'] = (6,4)\n", "\n", "np.random.seed(859303)\n", "\n", "# Vary ysigma from small to large values and see how the results change\n", "ysigma = 10\n", "\n", "x1 = np.random.uniform(0, 500, 1000)\n", "shift1 = np.random.normal(0,ysigma,1000)\n", "\n", "y1 = np.sqrt(x1)+shift1\n", "\n", "print(\"cov(x,y) for sigma(y) = \",ysigma,\"is: \",get_correlation_coeff(x1,y1))\n", "plt.title(\"y=sqrt(x)\")\n", "plt.scatter(x1,y1)\n", "plt.show()\n", "\n", "x2 = np.random.uniform(0, 4.5, 1000)\n", "shift2 = np.random.normal(0,ysigma,1000)\n", "y2 = x2*x2 + shift2\n", "\n", "print(\"cov(x,y) for sigma(y) = \",ysigma,\"is: \",get_correlation_coeff(x2,y2))\n", "plt.title(\"y=x*x\")\n", "plt.scatter(x2,y2)\n", "plt.show()\n", "\n", "x3 = np.random.uniform(0, 20, 1000)\n", "shift3 = np.random.normal(0,ysigma,1000)\n", "y3 = x3 + shift3\n", "\n", "print(\"cov(x,y) for sigma(y) = \",ysigma,\"is: \",get_correlation_coeff(x3,y3))\n", "plt.title(\"y=x\")\n", "plt.scatter(x3,y3)\n", "plt.show()\n", "\n", "#reset default figure size\n", "plt.rcParams['figure.figsize'] = (9,6)"]}, {"cell_type": "markdown", "id": "e47059fe", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Problem 8.3.3</span>\n", "\n", "We've studied the dependence of the correlation coefficient on the standard deviation for the equation `y(std) = x + unc`, where `unc` is an array of random uncertainties drawn from a normal distribution with mean 0 and the given standard deviation. We've also shown above that there are differences in the correlation coefficient for different functional forms. Let's now look at this issue in more detail.\n", "\n", "\n", "Specifically, how does the graph of `correlation_coefficient_given_std_dev` vs. `std` change with different functional forms such as `y(std) = 2x + unc` and `y(std) = x^2 + unc` when `1 < |x|`?\n", "\n", "\n", "For a given `std`, the correlation coefficient between `y` and `x` (compared to the case where `y(std) = x + unc`):\n", "\n", "- is larger for both `y=2x + err` and `y=x^2 + err`.\n", "- is smaller for both `y=2x + err` and `y=x^2 + err`.\n", "- is larger for `y=2x + err` and smaller for `y=x^2 + err`.\n", "- is smaller for `y=2x + err` and larger for `y=x^2 + err`.\n", "- is the same for both 'y=2x + err' and 'y=x^2 + err'.\n", "- the answer is different for different values of `std`.\n"]}, {"cell_type": "markdown", "id": "f2a0c9ce", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_8_4'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">P8.4 Covariance Matrix and Computational Examples</h2>   \n", "\n", "| [Top](#section_8_0) | [Previous Section](#section_8_3) | [Problems](#problems_8_4) |\n"]}, {"cell_type": "markdown", "id": "9232daf1", "metadata": {"tags": ["learner", "catsoop_04", "md"]}, "source": ["<h3>Overview</h3>\n", "\n", "In the previous sections where we've studied explicitly calculating the variance, covariance, and correlation coefficient, we've also pointed out that these quantities can be found directly using functions in the numpy library. \n", "\n", "In fact, you've seen the use of two functions, specifically `np.cov([x,y])[0,1]` and `np.corrcoef([x,y])[0,1]`, which calculate the covariance and correlation coefficient, respectively, between two datasets $x$ and $y$. The `[0,1]` indicates that these functions actually return matrices, for which we've only seen one element. Let's look at the values of the other elements of each matrix using one of the datafiles that were downloaded at the start of this notebook.\n"]}, {"cell_type": "code", "execution_count": null, "id": "f3af3efe", "metadata": {"tags": ["learner", "py", "learner_chopped", "catsoop_04"]}, "outputs": [], "source": ["#>>>RUN: P8.4-runcell01\n", "\n", "import numpy as np \n", "import matplotlib.pyplot as plt\n", "\n", "pts = np.genfromtxt('data/P07/slope_2.txt', delimiter=',') #read in example file, \n", "# play around with different example files, view the different file names in `data/P07`\n", "\n", "x = [n[0] for n in pts]    # get x values\n", "y = [n[1] for n in pts]    # get y values\n", "\n", "plt.scatter(x,y,s=5) #Plot the points from our example file\n", "plt.xlabel('x')\n", "plt.ylabel('y')\n", "plt.show()\n", "\n", "print('covariance matrix:')\n", "print(np.cov(x,y)) #Print out covariance of x,y\n", "\n", "print('correlation coefficient matrix:')\n", "print(np.corrcoef(x,y)) #Print out covariance of x,y"]}, {"cell_type": "markdown", "id": "0558eb6d", "metadata": {"tags": ["learner", "catsoop_04", "md"]}, "source": ["Since the second matrix can be calculated from the elements of the first one, we'll concentrate our discussion on the first one, the so-called \"covariance matrix\".\n", "\n", "For a set of $N$ datasets, $x_1,x_2,...,x_N$, the covariance matrix is an $N\\times N$ matrix where the diagonal entries are the variance or $\\sigma^2$ of the corresponding dataset, i.e. $M_{ii}=\\text{var}(x_i)=\\sigma_i^2$. Additionally, the off-diagonal values contain the covariance of two datasets, so $M_{ij}=\\sigma_{i,j}$. The covariance $\\sigma_{i,j}$ is always equivalent to $\\sigma_{j,i}$ so $M$ is a symmetric matrix. Shown visually we have for $3$ datasets $x,y$, and $z$:\n", "\n", "<br>\n", "\n", "$$\n", "M=\\begin{pmatrix} \\sigma_x^2 & \\sigma_{x,y} & \\sigma_{x,z}\\\\ \n", "\\sigma_{y,x} & \\sigma_{y}^2 & \\sigma_{y,z}\\\\ \\sigma_{z,x} & \\sigma_{z,y} & \\sigma_{z}^2 \\end{pmatrix}\n", "$$\n", "\n", "<br>\n", "\n", "For more than two datasets, the numpy syntax changes slightly, namely replacing the two arguments in parentheses seen above with three or more arguments in square brackets:"]}, {"cell_type": "code", "execution_count": null, "id": "15fd867c", "metadata": {"tags": ["learner", "py", "learner_chopped", "catsoop_04"]}, "outputs": [], "source": ["#>>>RUN: P8.4-runcell02\n", "\n", "np.random.seed(859303)\n", "\n", "#Generating our three datasets\n", "x = np.random.random(100)\n", "y = np.arange(100)+np.random.uniform(-2,2,size=(100,))\n", "delta = np.random.uniform(-1,1,size=(100,))\n", "z = .5*y+delta\n", "\n", "#Plotting our data\n", "ax = plt.axes(projection='3d')\n", "ax.scatter3D(x,y,z)\n", "plt.xlabel('x')\n", "plt.ylabel('y')\n", "plt.show()\n", "\n", "#Getting our covariance matrix, notice how we need a list for 3 datasets here instead of just two arguments\n", "M = np.cov([x,y,z])\n", "print('covariance matrix:')\n", "print(M)\n", "\n", "#Getting our correlation coefficient matrix, notice how we need a list for 3 datasets here instead of just two arguments\n", "M = np.corrcoef([x,y,z])\n", "print('correlation coeff matrix:')\n", "print(M)"]}, {"cell_type": "markdown", "id": "00b16d98", "metadata": {"tags": ["learner", "catsoop_04", "md"]}, "source": ["Visually, the 3D data display shown above doesn't appear to show any strong relationships between $x,y,$ and $z$. However, by examining the covariance matrix, we can see a large covariance value (~425) for $\\sigma_{y,z}=\\sigma_{z,y}$, suggesting that they are likely correlated. One needs to be cautious since both $y$ and $z$ have a very large extent which, as we've seen, can lead to a larger covariance. As shown in the second printout, $\\rho_{y,z}$ has a value of $0.999$, meaning these two datasets are, in fact, very strongly correlated! The correlation coefficients between $x$ and both $y$ and $z$ are much smaller.\n", "\n", "These relationships are much clearer in the individual 2D plots shown by the code below:\n"]}, {"cell_type": "code", "execution_count": null, "id": "0d482d1b", "metadata": {"tags": ["learner", "py", "learner_chopped", "catsoop_04"]}, "outputs": [], "source": ["#>>>RUN: P8.4-runcell03\n", "\n", "plt.rcParams['figure.figsize'] = (6,4)\n", "\n", "#Plotting our data\n", "plt.scatter(x,y,s=2)\n", "plt.xlabel('x')\n", "plt.ylabel('y')\n", "plt.show()\n", "\n", "#Plotting our data\n", "plt.scatter(x,z,s=2)\n", "plt.xlabel('x')\n", "plt.ylabel('z')\n", "plt.show()\n", "\n", "#Plotting our data\n", "plt.scatter(y,z,s=2)\n", "plt.xlabel('y')\n", "plt.ylabel('z')\n", "plt.show()\n", "\n", "#reset default figure size\n", "plt.rcParams['figure.figsize'] = (9,6)"]}, {"cell_type": "markdown", "id": "5d72745c", "metadata": {"tags": ["learner", "catsoop_04", "md"]}, "source": ["Something to think about: If $y$ and $z$ are strongly correlated, could $x$ have a large correlation coefficient with one of those variables, but not with both?"]}, {"cell_type": "markdown", "id": "4a0e9069", "metadata": {"tags": ["learner", "catsoop_04", "md"]}, "source": ["Using what we've learned about the covariance matrix and the correlation coefficient, let's use them to calculate some of the other quantities we've discussed:"]}, {"cell_type": "markdown", "id": "174e0a8d", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='problems_8_4'></a>   \n", "\n", "| [Top](#section_8_0) | [Restart Section](#section_8_4) |\n"]}, {"cell_type": "markdown", "id": "36ea3258", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Problem 8.4.1</span>\n", "\n", "Given a covariance matrix $M$ with $N$ datasets, $x_0,x_1,...,x_{N-1}$ , create three functions to find the following:\n", "\n", "* The function `std_i` which returns the standard deviation of dataset $i$\n", "* The function `cov_ij` which returns covariance of datasets $x_i$ and $x_j$, i.e. $\\sigma_{i,j}$\n", "* The function `corr_ij` which returns the correlation coefficient of datasets $x_i$ and $x_j$, i.e. $\\rho_{i,j}$\n", "\n", "Design your functions such that they take in 2-3 parameters, those being the covariance matrix $M$ and the indices requested (one index for `std_i` and 2 for `cov_ij` and `corr_ij`)."]}, {"cell_type": "code", "execution_count": null, "id": "123be276", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>PROBLEM: P8.4.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def std_i(M, i):\n", "  #INSERT CODE HERE\n", "\n", "def cov_ij(M, i, j):\n", "  #INSERT CODE HERE\n", "\n", "def corr_ij(M, i, j):\n", "  #INSERT CODE HERE\n", "\n", "np.random.seed(7)\n", "array_size = 100\n", "x = np.random.random(array_size)\n", "y = np.arange(array_size)+np.random.uniform(-2,2,size=(array_size,))\n", "delta = np.random.uniform(-1,1,size=(array_size,))\n", "z = .5*y+delta\n", "\n", "M = np.cov([x,y,z])\n", "\n", "print('stdev_i for i=[0,1,2]:',std_i(M, 0), std_i(M, 1), std_i(M, 2))\n", "print('cov_ij for ij=[[0,2],[1,2]]:',cov_ij(M, 0, 2), cov_ij(M, 1, 2))\n", "print('corr_ij for ij=[[0,2],[1,2]]:',corr_ij(M, 0, 2), corr_ij(M, 1, 2))\n"]}, {"cell_type": "markdown", "id": "cfa0717b", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Problem 8.4.2</span>\n", "\n", "In the code shown below, you are given 5 variables, $v$, $w$, $x$, $y$, and $z$. You might be tempted to think of them as four independent variables and one dependent one: $z=f(v,w,x,y)$. However, in reality, $v$, $w$ ,$x$, and $y$ are all interdependent on each other as well. Using the covariant matrix and the three functions you defined in the previous problem, rank the variables $v$, $w$, $x$, and $y$ in order of least to most strongly correlated with $z$.\n", "\n", "Enter your answer as a list of the **lowercase** variable names $v$, $w$, $x$, and $y$ `[v1, v2, v3, v4]` where `v1` is the most weakly correlated with $z$ and `v4` is the most strongly correlated."]}, {"cell_type": "code", "execution_count": null, "id": "2b2e32f2", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>PROBLEM: P8.4.2\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "np.random.seed(859303)\n", "\n", "v = np.random.random(100)\n", "w = np.arange(100)+np.random.uniform(-2,2,size=(100,))\n", "x = (np.random.random(100) + v)/2\n", "y = (np.arange(100)+np.random.uniform(-2,2,size=(100,)))/(w+x+2)\n", "delta = np.random.uniform(-1,1,size=(100,))\n", "z = .5*y+delta + .05* w\n", "\n", "################################\n", "####### INSERT CODE HERE #######\n", "################################\n"]}, {"cell_type": "markdown", "id": "efec0cfb", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": [">#### Follow-up 8.4.2a (ungraded)\n", ">\n", ">At this point in the course, we have derived a small set of parameters which tell us a lot of information about the distributions we are looking at. However, our visual intuition often gives us more insights than just the statistical parameters of a dataset (or datasets). Therefore, we continue to strive for more sophisticated algorithms to help understand various aspects of datasets.\n", ">\n", ">To showcase how little we should rely solely on statistical parameters, Alberto Cairo created the Datasaurus dataset. The full paper can be found <a href=\"https://damassets.autodesk.net/content/dam/autodesk/research/publications-assets/pdf/same-stats-different-graphs.pdf\" target=\"_blank\">here</a> and the data are found <a href=\" http://www.thefunctionalart.com/2016/08/download-datasaurus-never-trust-summary.html\" target=\"_blank\">here</a>, but below you can find a quick implementation (which relies on the function you have written previously) of how very different images can have surprisingly similar statistical parameters."]}, {"cell_type": "code", "execution_count": null, "id": "d060f6d7", "metadata": {"tags": ["py", "learner_chopped", "learner"]}, "outputs": [], "source": ["#>>>RUN: P8.4.2a\n", "\n", "data = pd.read_csv('data/P07/DatasaurusDozen.tsv', sep='\\t')\n", "\n", "data_dictionary = {}\n", "for image_name in data['dataset'].unique():\n", "    x = np.array(data[data.dataset==image_name]['x'])\n", "    y = np.array(data[data.dataset==image_name]['y'])\n", "    plt.scatter(x, y)\n", "    plt.show()\n", "    print(\"x mean: \", np.mean(x))\n", "    print(\"y mean: \", np.mean(y))\n", "    print(\"x variance: \", get_variance(x))\n", "    print(\"y variance: \", get_variance(y))\n", "    print(\"Correlation coefficient: \", get_correlation_coeff(x,y))"]}], "metadata": {"celltoolbar": "Tags", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}