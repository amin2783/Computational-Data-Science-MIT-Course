{"cells": [{"cell_type": "markdown", "id": "2369d70f", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<hr style=\"height: 1px;\">\n", "<i>This notebook was authored by the 8.S50x Course Team, Copyright 2022 MIT All Rights Reserved.</i>\n", "<hr style=\"height: 1px;\">\n", "<br>\n", "\n", "<h1>Guided Problem Set 5: Likelihoods and the $\\chi^2$ Distribution</h1>\n"]}, {"cell_type": "markdown", "id": "0bfcb400", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_5_0'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">P5.0 Overview</h2>\n"]}, {"cell_type": "markdown", "id": "25ddae8d", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<h3>Navigation</h3>\n", "\n", "<table style=\"width:100%\">\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_5_1\">P5.1 Warm-up Exercise</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#problems_5_1\">P5.1 Problems</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_5_2\">P5.2 The Likelihood Function</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#problems_5_2\">P5.2 Problems</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_5_3\">P5.3 Maximum Likelihood and Chi-Squared</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#problems_5_3\">P5.3 Problems</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_5_4\">P5.4 A Fitting Example</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#problems_5_4\">P5.4 Problems</a></td>\n", "    </tr>\n", "</table>\n", "\n"]}, {"cell_type": "markdown", "id": "b8d8f05d", "metadata": {"tags": ["learner", "catsoop_00", "md"]}, "source": ["<h3>Learning Objectives</h3>\n", "\n", "In this recitation we will explore the following objectives:\n", "\n", "- Understand and construct likelihood and log-likelihood functions\n", "- Examine the $\\chi^2$ metric\n", "- Perform a fit by maximizing a log-likelihood function using `lmfit`\n", "\n"]}, {"cell_type": "markdown", "id": "ec62e79a", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<h3>Importing Data (Colab Only)</h3>\n", "\n", "If you are in a Google Colab environment, run the cell below to import the data for this notebook. Otherwise, if you have downloaded the course repository, you do not have to run the cell below."]}, {"cell_type": "code", "execution_count": null, "id": "cd866f44", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: P5.0-runcell00\n", "\n", "!git init\n", "!git remote add -f origin https://github.com/mitx-8s50/nb_LEARNER/\n", "!git config core.sparseCheckout true\n", "!echo 'data/P05' >> .git/info/sparse-checkout\n", "!git pull origin main\n"]}, {"cell_type": "markdown", "id": "f7bcb6bf", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Importing Libraries</h3>\n", "\n", "Before beginning, run the cell below to import the relevant libraries for this notebook."]}, {"cell_type": "code", "execution_count": null, "id": "bcfe3501", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: P5.0-runcell01\n", "\n", "#install lmfit if you have not done so yet\n", "!pip install lmfit"]}, {"cell_type": "code", "execution_count": null, "id": "e88e5357", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: P5.0-runcell02\n", "\n", "import numpy as np                 #https://numpy.org/doc/stable/\n", "import matplotlib.pyplot as plt    #https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.html\n", "from mpl_toolkits import mplot3d   #https://matplotlib.org/2.0.2/mpl_toolkits/mplot3d/tutorial.html\n", "\n", "from scipy.stats import chisquare  #https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chisquare.html\n", "from scipy.stats import poisson    #https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.poisson.html\n", "from scipy.stats import norm       #https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html\n", "\n", "import lmfit\n", "from lmfit import Parameters, minimize, fit_report  #https://lmfit.github.io/lmfit-py/parameters.html\n", "                                                    #https://lmfit-py.readthedocs.io/en/latest/fitting.html#the-minimize-function\n", "                                                    #https://lmfit-py.readthedocs.io/en/latest/fitting.html#getting-and-printing-fit-reports"]}, {"cell_type": "markdown", "id": "24e7257c", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Setting Default Figure Parameters</h3>\n", "\n", "The following code cell sets default values for figure parameters."]}, {"cell_type": "code", "execution_count": null, "id": "3d53a02b", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: P5.0-runcell03\n", "\n", "#set plot resolution\n", "%config InlineBackend.figure_format = 'retina'\n", "\n", "#set default figure parameters\n", "plt.rcParams['figure.figsize'] = (9,6)\n", "\n", "medium_size = 12\n", "large_size = 15\n", "\n", "plt.rc('font', size=medium_size)          # default text sizes\n", "plt.rc('xtick', labelsize=medium_size)    # xtick labels\n", "plt.rc('ytick', labelsize=medium_size)    # ytick labels\n", "plt.rc('legend', fontsize=medium_size)    # legend\n", "plt.rc('axes', titlesize=large_size)      # axes title\n", "plt.rc('axes', labelsize=large_size)      # x and y labels\n", "plt.rc('figure', titlesize=large_size)    # figure title"]}, {"cell_type": "markdown", "id": "c52710db", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_5_1'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">P5.1 Warm-up Exercise</h2>    \n", "\n", "| [Top](#section_5_0) | [Previous Section](#section_5_0) | [Problems](#problems_5_1) | [Next Section](#section_5_2) |\n"]}, {"cell_type": "markdown", "id": "6e5facfb", "metadata": {"tags": ["learner", "catsoop_01", "md"]}, "source": ["<h3>Overview</h3>\n", "\n", "Let $x_i$ be the x coordinates of the data points we observe, and let $f(x_i)$ be the model we are trying to fit. We assume that the data that we observe is generated from this model with some extra error, usually due to some sort of noise.\n", "\n", "$$y_i = f(x_i) + \\epsilon_{i} $$\n", "\n", "where $\\epsilon_{i}$ is a set of random variables drawn from a Gaussian distribution with a mean of 0. In some physics experiments, we will be able to quantify the width of the $\\epsilon$ distribution by making repeated measurements and calculating the standard errors,  $\\sigma_i$, which are usually plotted as error bars like in the code below."]}, {"cell_type": "code", "execution_count": null, "id": "baa8e890", "metadata": {"tags": ["learner", "py", "catsoop_01", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: P5.1-runcell01\n", "\n", "np.random.seed(9)\n", "\n", "y_err = 1. #play with changing this, what does the graph look like?\n", "\n", "x = np.arange(0,9, .5)\n", "y_true = x \n", "y_data = np.add(np.random.normal(size=len(x),scale=y_err), y_true) #y=x+e where e is Gaussian noise\n", "err = np.ones(len(x))*y_err #constant error\n", "\n", "plt.errorbar(x,y_data,err, fmt = 'o', label='data') #plot the data\n", "plt.plot(x, y_true, color='orange', label='f(x)') #plot the line y=x\n", "plt.legend(loc=2)\n", "plt.ylabel('y')\n", "plt.xlabel('x')\n", "plt.show()"]}, {"cell_type": "markdown", "id": "37f3908a", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='problems_5_1'></a>     \n", "\n", "| [Top](#section_5_0) | [Restart Section](#section_5_1) | [Next Section](#section_5_2) |\n"]}, {"cell_type": "markdown", "id": "c46a4bcf", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Problem 5.1.1</span>\n", "\n", "We assume that $y_i$ is a random variable with mean $f(x_i)$, and variance $\\sigma_i^2$. Given this, what is the probability distribution for $y_i$? Insert the formula in the function definition below. You can optionally plot this distribution, for a given $f(x_i)$ value that we call `y_meani`."]}, {"cell_type": "code", "execution_count": null, "id": "e008f8b2", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>PROBLEM: P5.1.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def P(yi, f_xi, sigma):\n", "    return #INSERT CODE HERE\n", "\n", "\n", "#Setting the mean and variance of the random variable (PLAY WITH THESE!)\n", "mean = 0\n", "variance = 1\n", "\n", "#Creating PDF distribution\n", "y_array = np.arange(-4, 4, .01)\n", "y_dist = P(y_array, mean, np.sqrt(variance))\n", "\n", "#Plotting\n", "plt.xlabel(\"yi\")\n", "plt.ylabel(\"PDF\")\n", "plt.plot(y_array,y_dist)\n", "plt.show()"]}, {"cell_type": "markdown", "id": "0047e921", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": [">#### Follow-up 5.1.1a (ungraded)\n", ">\n", ">Try plotting a histrogram of $y_i - f(x_i)$. What should be the shape of this distribution? Does this match your expectation? Try varying `y_err` and the number of data points that are generated.\n", ">\n", ">Also try plotting a histogram of $(y_i - f(x_i))/\\sigma_{i}$. What was this distribution called?"]}, {"cell_type": "markdown", "id": "0260a924", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Problem 5.1.2</span>\n", "\n", "Given the answer to the previous problem, and assuming that the probability distributions for each of the $y_i$ are independent, what is the joint probability distribution of two random variables $y_1$ and $y_2$? Complete the code below and submit your answer (you can simplify your answer by using the function `P(yi, f_xi, sigma)` that your already defined).\n", "\n", "Afterwards, run the completed code, which makes a 3D plot of the joint probability distribution with one axis as $y_1$ and the other $y_2$. Once you have a working solution, try playing around with the means, variances, and ranges to see how the plot changes.\n", "\n", "You may also find it instructive to read the documentation:\n", "\n", "- <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.meshgrid.html\" target=\"_blank\">`np.meshgrid`</a>\n", "- <a href=\"https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.figure.html\" target=\"_blank\">`plt.figure`</a>\n", "- <a href=\"https://matplotlib.org/2.0.2/mpl_toolkits/mplot3d/tutorial.html\" target=\"_blank\">`mplot3d`</a>"]}, {"cell_type": "code", "execution_count": null, "id": "f53075be", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>PROBLEM: P5.1.2\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def P_2D(y1, f_x1, sigma1, y2, f_x2, sigma2):\n", "    return #INSERT CODE HERE\n", "\n", "#Setting the mean and variance of the random variables (PLAY WITH THESE!)\n", "mean1 = 0\n", "variance1 = 1\n", "mean2 = 0\n", "variance2 = 1\n", "\n", "#Creating PDF distribution\n", "y1 = np.arange(-4, 4, .01)\n", "y2 = np.arange(-4, 4, .01)\n", "Y1, Y2 = np.meshgrid(y1, y2)\n", "Y = P_2D(Y1, mean1, np.sqrt(variance1), Y2, mean2, np.sqrt(variance2))\n", "\n", "#Plotting\n", "fig = plt.figure()\n", "ax = plt.axes(projection='3d')\n", "ax.contour3D(y1, y2, Y, 100)\n", "ax.set_xlabel('y1')\n", "ax.set_ylabel('y2')\n", "ax.set_zlabel('PDF')\n", "plt.show();\n"]}, {"cell_type": "markdown", "id": "930b7627", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Problem 5.1.3</span>\n", " \n", "Suppose we again combine two independent variables, $y_1$ and $y_2$, but this time $y_1$ and $y_2$ are not taken from Gaussian distributions, but instead from two different, more exotic, distributions (i.e. the functional forms of $P(y_{1})$ and $P(y_{2})$ are not the same). Would it still be necessarily true that $P(y_1, y_2) = P(y_1) P(y_2)$?\n"]}, {"cell_type": "markdown", "id": "c823b0a7", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_5_2'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">P5.2 The Likelihood Function</h2>    \n", "\n", "| [Top](#section_5_0) | [Previous Section](#section_5_1) | [Problems](#problems_5_2) | [Next Section](#section_5_3) |\n"]}, {"cell_type": "markdown", "id": "6505c988", "metadata": {"tags": ["learner", "catsoop_02", "md"]}, "source": ["<h3>Overview</h3>\n", "\n", "\n", "The previous exercise gives the probability distribution of observing the data $y$ given a particular model $f(x)$ as a function of the independent data points $x$ and the function's parameters $\\alpha_1, \\alpha_2, \\alpha_3, ..., \\alpha_m $. This is known as the <b>likelihood</b> function $P(y|\\alpha)$. Note that the probability of observing the data given a particular model is *not* the same as the probability of a particular model being true given observed data (although they are related). To begin, however, we will assume that if we find the model that maximizes the probability of the observed data, this model is close to the true model. This technique is called <b>Maximum Likelihood Estimation</b> (MLE).\n", "\n", "Suppose we take the errors $\\epsilon$ in $y_i = f(x_i; \\alpha) + \\epsilon $ to be sampled from some general distribution $\\rho(y_i - f(x_i, \\alpha))$ instead of a Gaussian. We can then construct a  likelihood $P(y|\\alpha)$ (and corresponding log-likelihood) function:\n", "\n", "$$ P(y|\\alpha) = \\prod_i \\rho(y_i - f(x_i; \\alpha))$$\n", "\n", "Maximizing this likelihood with respect to the set of parameters $\\alpha$ will give us the parameterization of the model that best fits the given data. However, it is often more convenient (and more common) to maximize $\\log(P(y|\\alpha))$ instead."]}, {"cell_type": "markdown", "id": "c7cecf40", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='problems_5_2'></a>     \n", "\n", "| [Top](#section_5_0) | [Restart Section](#section_5_2) | [Next Section](#section_5_3) |\n"]}, {"cell_type": "markdown", "id": "3ab1dcee", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Problem 5.2.1</span>\n", "\n", "\n", "Why is it okay to maximize the log likelihood instead of the likelihood in order to find the fit function parameters that most closely describe the data? Choose the best answer from the options below.\n", "\n", "Hint: What is the only purpose of the likelihood function that has been mentioned so far? How is it affected by taking the log?\n", "\n", "\n", "A. The log of any general likelihood function is the same as the function itself\n", "\n", "B. Maximizing the log of the likelihood is the same as maximizing the likelihood\n", "\n", "C. Once we find the parameters that maximize the log of the likelihood function, we can take the inverse log (also know as exponentiating) of those parameters to get the parameters that maximize the likelihood function itself.\n", "\n", "D. The log of the likelihood will be close enough to the likelihood that we can just approximate them to be the same and then run all of our analysis on the log of the likelihood\n"]}, {"cell_type": "markdown", "id": "a970d9f6", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Problem 5.2.2</span>\n", "\n", "To convince yourself that your previous answer is correct, take the log of the arbitrary function defined in the code cell below, and then find the $(x, y)$ position of its maximum. Compare this result to the $(x, y)$ position of the maximum of the original function.\n", "\n", "Enter the $x$ value where the original function has its maximum, `x_orig`, and the $x$ value where the log-function has its maximum, `x_log`, as a list `[x_orig,x_log]`, with precision 1e-2."]}, {"cell_type": "code", "execution_count": null, "id": "18b279e8", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>PROBLEM: P5.2.2\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "\n", "def find_maximum(x,y):\n", "  #Function that takes in two arrays, representing the input of a function,x,\n", "  #and the output of that function, y, and returns the (x,y) coordinate of\n", "  #the maximum as a tuple\n", "  max_y = max(y)\n", "  max_x = #INSERT CODE HERE\n", "  return (max_x,max_y)\n", "\n", "def arbitrary_function(x):\n", "    return 2*(2*np.sin(x) + 3*np.sin(2*x) + .5 * np.sin(10*x) + 3*np.sin(.3*x)) -.1*x**2 + x + 2\n", "\n", "def log_arbitrary_function(x):\n", "    return #INSERT CODE HERE\n", "\n", "x = np.arange(0, 10, .01)\n", "\n", "y = arbitrary_function(x)\n", "print(\"Maximum of arbitrary function: \", find_maximum(x,y))\n", "plt.plot(x, y)\n", "plt.show()\n", "\n", "###########################################################\n", "###### INSERT CODE HERE TO PLOT THE LOG ARBITRARY.  #######\n", "###### FUNCTION AND PRINT ITs MAXIMUM X AND Y COORD #######\n", "###########################################################\n", "\n"]}, {"cell_type": "markdown", "id": "b5ff9f8c", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Problem 5.2.3</span>\n", "\n", "Is the $y$ coordinate of the log of the arbitrary function at its maximum point the same as the $y$ maximum of the original arbitrary function?\n"]}, {"cell_type": "markdown", "id": "506978bc", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_5_3'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">P5.3 Maximum Likelihood and Chi-Squared</h2>    \n", "\n", "| [Top](#section_5_0) | [Previous Section](#section_5_2) | [Problems](#problems_5_3) | [Next Section](#section_5_4) |\n"]}, {"cell_type": "markdown", "id": "c8353ae0", "metadata": {"tags": ["learner", "catsoop_03", "md"]}, "source": ["<h3>Maximum Likelihood Estimation for Gaussian Errors</h3>\n", "\n", "Let's return to having $\\epsilon$ in $y_i = f(x_i) + \\epsilon $ be a Gaussian distribution. We would like to find the parameters that maximize the likelihood distribution,  i.e.\n", "\n", "$$\\alpha^{\\text{best}}_j = \\text{argmax}_{\\alpha_j} \\left( \\frac{1}{(2\\pi)^{k/2}\\prod_i\\sigma_i} \\exp\\left( -\\frac{1}{2} \\sum_i\\frac{(y_i - f(x_i; \\alpha_1, \\alpha_2, ...))^2}{\\sigma_i^2}\\right)\\right)$$\n", "\n", "We know that since the logarithm is a monotonically increasing function, if we maximize the logarithm of this function, we maximize the function itself. This means that\n", "\n", "$$\\alpha^{\\text{best}}_j = \\text{argmax}_{\\alpha_j} \\left( -\\frac{1}{2} \\sum_i\\frac{(y_i - f(x_i; \\alpha_1, \\alpha_2, ...))^2}{\\sigma_i^2} - \\sum_i \\log (\\sigma_i\\sqrt{2\\pi}) \\right)$$\n", "\n", "\n", "The term involving the $\\sum_i \\log (\\sigma_i\\sqrt{2\\pi})$ is a constant, so we can ignore it in the maximization. \n", "\n", "\n", "$$\\alpha^{\\text{best}}_j = \\text{argmax}_{\\alpha_j} \\left( -\\frac{1}{2} \\sum_i\\frac{(y_i - f(x_i; \\alpha_1, \\alpha_2, ...))^2}{\\sigma_i^2}  \\right)$$\n", "\n", "\n", "Similarly, the factor of $-\\frac{1}{2}$ in front is also a constant. However, we cannot simply ignore it because it has a negative value, If we want to **maximize** a function $-\\frac{1}{2}g(z)$, we need to **minimize** the function $g(z)$. So, this gives us the following:\n", "\n", "\n", "$$\\alpha^{\\text{best}}_j = \\text{argmin}_{\\alpha_j} \\left(\\sum_i\\frac{(y_i - f(x_i; \\alpha_1, \\alpha_2, ...))^2}{\\sigma_i^2}  \\right)$$\n", "\n"]}, {"cell_type": "markdown", "id": "4016e023", "metadata": {"tags": ["learner", "catsoop_03", "md"]}, "source": ["<h3>The Chi-squared Function</h3>\n", "\n", "The maximum likelihood solution for the parameters can be found by minimizing the sum of squared residuals divided by the variances. This sum is commonly referred to as the $\\chi^2$, defined as:\n", "\n", "\n", "$$\\chi^2(\\alpha_1, \\alpha_2 ... \\alpha_m) = \\sum_i\\frac{(y_i - f(x_i; \\alpha_1, \\alpha_2, ...))^2}{\\sigma_i^2}$$\n", "\n", "\n", "**So, many fitting problems that you encounter come down to minimizing this sum!**\n", "\n", "In an earlier Lesson, we had a similar sum, but without the $\\sigma_i^2$ in the denominator. If we assume that each point has the same uncertainty, then minimizing this $\\chi^2$ is the same as minimizing the sum we saw earlier. The quantity shown above is known as the <b>weighted least squares</b>, as opposed to what was used the earlier derivation, which is the <b>ordinary least squares</b>."]}, {"cell_type": "markdown", "id": "8bab0b55", "metadata": {"tags": ["learner", "catsoop_03", "md"]}, "source": ["<h3>Numerical Minimization</h3>\n", "\n", "In an earlier Lesson, we saw how the minimization can be solved analytically, using linear algebra or vector calculus. However, this can get very complicated for anything other than very simple fit functions. As a more versatile alternative, we can use the computer to numerically minimize the $\\chi^2$. The most commonly-used minimization technique is known as gradient descent, which will be covered in one of the later Lessons. Gradient descent is what the Python package `lmfit` uses."]}, {"cell_type": "markdown", "id": "452a0d16", "metadata": {"tags": ["learner", "catsoop_03", "md"]}, "source": ["<h3>What does Chi-squared tell us?</h3>\n", "\n", "For now we will not discuss the minimization itself, but instead delve deeper into the significance of the $\\chi^{2}$, in particular how we can use it to assess the overall quality (often called goodness) of a fit. \n", "\n", "Let's see what the $\\chi^2$ can tell us when comparing data to a model. First, run the code below to read model points from a file and plot them as a histogram."]}, {"cell_type": "code", "execution_count": null, "id": "8a02a699", "metadata": {"tags": ["learner", "py", "catsoop_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: P5.3-runcell01\n", "\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "from scipy.stats import chisquare\n", "\n", "# model\n", "pred = np.genfromtxt('data/P05/chisq_model.txt', delimiter=',')    # read in file\n", "x = np.array([n[0] for n in pred])    # get x values\n", "pred_y = np.array([n[1] for n in pred])    # get y values\n", "plt.xlim(-1,1)\n", "plt.xlabel(r'$x$')\n", "plt.ylabel(r'Frequency')\n", "plt.plot(x,pred_y,linewidth=1,drawstyle='steps')\n", "plt.show()\n"]}, {"cell_type": "markdown", "id": "82b30e36", "metadata": {"tags": ["learner", "catsoop_03", "md"]}, "source": ["**Data Sample 1**\n", "\n", "Next, read the data points in the file \"chisq_data1.txt\" and plot them (with error bars) on top of the model histogram. Note that this data file does not contain uncertainties. As you can see in the function `plot_error`, we assume that the uncertainties are Poisson, so we can calculate them as simply the square root of the number of counts for each data point.\n"]}, {"cell_type": "code", "execution_count": null, "id": "fc0fc70e", "metadata": {"tags": ["learner", "py", "catsoop_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: P5.3-runcell02\n", "\n", "def plot_error(y):\n", "    plt.errorbar(x,y,yerr=np.sqrt(y),ecolor='k',elinewidth=1,capsize=4,linestyle='')"]}, {"cell_type": "code", "execution_count": null, "id": "c6f8c915", "metadata": {"tags": ["learner", "py", "catsoop_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: P5.3-runcell03\n", "\n", "data1 = np.loadtxt('data/P05/chisq_data1.txt', delimiter=',')    # read in file\n", "plt.xlim(-1,1)\n", "plt.xlabel(r'$x$')\n", "plt.ylabel(r'Frequency')\n", "plt.plot(x,pred_y,linewidth=1,drawstyle='steps')\n", "plt.scatter(x,data1,c='k')\n", "plot_error(data1)"]}, {"cell_type": "markdown", "id": "b2a3fcb9", "metadata": {"tags": ["learner", "catsoop_03", "md"]}, "source": ["Compute and print the $\\chi^2$ coefficient and its probability. Note that we first need to normalize our model to our data in order to calculate the correct $\\chi^2$ value. \n", "\n", "For finding the $\\chi^2$ coefficient and its probability, we will use the `scipy.chisquare` function, which takes data and a prediction and computes the $\\chi^{2}$. This computation makes the assumption that the uncertainty in the prediction is Poisson, namely $\\sigma_{i}=\\sqrt{y_{\\rm pred}}$. \n", "\n", "In the code cell below, we first check that we understand what is going on by taking two data and prediction points and calculating their $\\chi^{2}$. Given the assumption of Poisson predictions, this should give:\n", "\n", "$$\\chi^2 =\\sum_{i=1}^2\\frac{({\\rm data}_i - {\\rm pred}_i)^2}{\\sigma_i^2}= \\sum_{i=1}^2\\frac{({\\rm data}_i - {\\rm pred}_i)^2}{{\\rm pred}_i}$$\n", "\n", "You can see that the $\\chi^{2}$ returned by `chisquare` (the quantity labeled \"statistic =\") and the explicit calculation are identical.\n", "\n", "The `chisquare` function also outputs the probability of randomly getting a $\\chi^2$ greater than or equal to the value observed. To find this probability, it assumes that the residuals are all perfectly Gaussian. We will discuss how this probability computation is done later.  \n"]}, {"cell_type": "code", "execution_count": null, "id": "dcfd8e1a", "metadata": {"tags": ["learner", "py", "catsoop_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: P5.3-runcell04\n", "\n", "#quick test whether we understand what is going on\n", "print(\"Check:\",chisquare([10,20],[9.9,20.1]))\n", "\n", "print(\"This should be : \", (0.1**2/9.9 + 0.1**2/20.1))\n", "print()\n", "\n", "pred_y = pred_y/np.sum(pred_y) * np.sum(data1) #Normalize model to data\n", "#print(pred_y)\n", "\n", "chi2,p = chisquare(data1,pred_y)\n", "print(\"Chi-squared:\",chi2)\n", "print(\"Probability: {:.4%}\".format(p)) #a bit dodgy!\n", "\n", "\n"]}, {"cell_type": "markdown", "id": "0f9d6ba1", "metadata": {"tags": ["learner", "catsoop_03", "md"]}, "source": ["For this case, you can see in the plot that a very large fraction of the data points are 1$\\sigma$ or more away from the prediction. So, it's not surprising that the $\\chi^{2}$ is large and has a very low probability."]}, {"cell_type": "markdown", "id": "554eadbb", "metadata": {"tags": ["learner", "catsoop_03", "md"]}, "source": ["**Data Sample 2**\n", "\n", "Try again for a different set of data, called \"chisq_data2.txt\". Again, compute and print the Chi-squared coefficient and probability."]}, {"cell_type": "code", "execution_count": null, "id": "306efde7", "metadata": {"tags": ["learner", "py", "catsoop_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: P5.3-runcell05\n", "\n", "data2 = np.loadtxt('data/P05/chisq_data2.txt', delimiter=',')    # read in file\n", "plt.xlim(-1,1)\n", "plt.xlabel(r'$x$')\n", "plt.ylabel(r'Frequency')\n", "plt.plot(x,pred_y,linewidth=1,drawstyle='steps')\n", "plt.scatter(x,data2,c='k')\n", "plot_error(data2)\n", "plt.show()\n", "\n", "pred_y = pred_y/np.sum(pred_y) * np.sum(data2) #Normalize model to data\n", "chi2,p = chisquare(data2,pred_y)\n", "print(\"Chi-squared:\",chi2)\n", "print(\"Probability: {:.2%}\".format(p)) #quite consistent!"]}, {"cell_type": "markdown", "id": "d8056c9d", "metadata": {"tags": ["learner", "catsoop_03", "md"]}, "source": ["Here, you see a much larger fraction of the data points falling less than 1$\\sigma$ from the prediction, so the $\\chi^{2}$ is smaller and has a larger probability."]}, {"cell_type": "markdown", "id": "8bb1fa3a", "metadata": {"tags": ["learner", "catsoop_03", "md"]}, "source": ["**Data Sample 3**\n", "\n", "Finally, see what happens with a third set of data, called \"chisq_data3.txt\". Again, compute and print the $\\chi^2$ coefficient and its probability."]}, {"cell_type": "code", "execution_count": null, "id": "9724f323", "metadata": {"tags": ["learner", "py", "catsoop_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: P5.3-runcell06\n", "\n", "data3 = np.loadtxt('data/P05/chisq_data3.txt', delimiter=',')    # read in file\n", "plt.xlim(-1,1)\n", "plt.xlabel(r'$x$')\n", "plt.ylabel(r'Frequency')\n", "plt.plot(x,pred_y,linewidth=1,drawstyle='steps')\n", "plt.scatter(x,data3,c='k')\n", "plot_error(data3)\n", "\n", "pred_y = pred_y/np.sum(pred_y) * np.sum(data3) #Normalize model to data\n", "chi2,p = chisquare(data3,pred_y)\n", "print(\"Chi-squared:\",chi2)\n", "print(\"Probability: {:.2%}\".format(p)) #too consistent..."]}, {"cell_type": "markdown", "id": "af77fe42", "metadata": {"tags": ["learner", "catsoop_03", "md"]}, "source": ["Here, the probability of the observed $\\chi^{2}$ value is very high. "]}, {"cell_type": "markdown", "id": "8b7a5797", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='problems_5_3'></a>     \n", "\n", "| [Top](#section_5_0) | [Restart Section](#section_5_3) | [Next Section](#section_5_4) |\n"]}, {"cell_type": "markdown", "id": "46edb907", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Problem 5.3.1</span>\n", "\n", "In physics, the distributions that we measure are sometimes too complicated to be modeled with a straight forward analytical fit function. In such cases, our model (which we compare our real data to) will be a set of simulated data. \n", "\n", "For this problem, you are provided the $y$ value arrays for a real dataset (`data`) and a simulated one (`MC`). Compute the $\\chi^2$ value of the data as compared to the simulation and enter your answer as a number with precision 1e-3. *Remember that it is important to normalize the simulation to the data so that they have the same total number of events.*\n"]}, {"cell_type": "code", "execution_count": null, "id": "7c668936", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>PROBLEM: P5.3.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "bin_edges = np.array([  0.  ,   3.75,   7.5 ,  11.25,  15.  ,  18.75,  22.5 ,  26.25,\n", "        30.  ,  33.75,  37.5 ,  41.25,  45.  ,  48.75,  52.5 ,  56.25,\n", "        60.  ,  63.75,  67.5 ,  71.25,  75.  ,  78.75,  82.5 ,  86.25,\n", "        90.  ,  93.75,  97.5 , 101.25, 105.  , 108.75, 112.5 , 116.25,\n", "       120.  , 123.75, 127.5 , 131.25])\n", "\n", "bin_centers = np.array([(bin_edges[i]+bin_edges[i+1])/2 for i in range(len(bin_edges)-1)])\n", "\n", "data = np.array([ 403, 1114, 1345, 1379, 1236, 1056,  865,  659,  506,  380,  319,\n", "        178,  163,  119,   89,   54,   57,   37,   24,   19,   19,   19,\n", "          5,    9,    6,    4,    7,    3,    3,    2,    3,    0,    0,\n", "          4,    2])\n", "\n", "#Monte Carlo Simulation\n", "MC = np.array([3057, 7241, 9282, 9404, 8559, 7564, 6259, 5124, 3976, 3095, 2445,\n", "       1816, 1398, 1019,  763,  569,  399,  307,  194,  174,  117,   96,\n", "         67,   60,   29,   20,   22,   13,    7,    6,    5,    5,    2,\n", "          3,    3])\n", "\n", "\n", "#YOUR CODE HERE\n", "#Normalize model to data and print the chisquare result\n", "\n", "\n", "#NOW PLOTTING\n", "plt.bar(bin_centers, MC, width=3.75, color='red') #Plot MC\n", "plt.errorbar(bin_centers,data,yerr=np.sqrt(data),marker='o',linestyle='none') #Plot data\n", "plt.ylabel(\"Number of entries\")\n", "plt.show();\n", "\n", "plt.plot(np.arange(0, 135, .01), np.ones(len(np.arange(0, 135, .01))), color='black')\n", "plt.errorbar(bin_centers, data/MC,yerr=np.sqrt(data)/MC,marker='o',linestyle='none') #Plot ratio of data/MC (should be 1)\n", "plt.ylabel(\"Data/Simulation\")\n", "plt.ylim(0,2)\n", "plt.show();"]}, {"cell_type": "markdown", "id": "1315a402", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": [">#### Follow-up 5.3.1a (ungraded)\n", ">\n", ">Does the p-value make sense for this data? What could be wrong?"]}, {"cell_type": "markdown", "id": "46bacf13", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Problem 5.3.2</span>\n", "\n", "If you haven't done so, make the the first plot in the preceding problem, which compares a histogram of the normalized prediction to data points with error bars.\n", "\n", "Looking closely, you will notice that the peak in the data is a bit narrower than that for the simulation, resulting in very poor agreement between the two. Luckily, this dataset has a correction to account for this difference.\n", "\n", "Use the new `data_corr` histogram in the code cell below and again perform a $\\chi^2$ test. What is the p-value for this new set of data? Enter your answer as a number with precision 1e-3.\n", "\n"]}, {"cell_type": "code", "execution_count": null, "id": "3b7d160f", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>PROBLEM: P5.3.2\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "bin_edges = np.array([  0.  ,   3.75,   7.5 ,  11.25,  15.  ,  18.75,  22.5 ,  26.25,\n", "        30.  ,  33.75,  37.5 ,  41.25,  45.  ,  48.75,  52.5 ,  56.25,\n", "        60.  ,  63.75,  67.5 ,  71.25,  75.  ,  78.75,  82.5 ,  86.25,\n", "        90.  ,  93.75,  97.5 , 101.25, 105.  , 108.75, 112.5 , 116.25,\n", "       120.  , 123.75, 127.5 , 131.25])\n", "\n", "bin_centers = np.array([(bin_edges[i]+bin_edges[i+1])/2 for i in range(len(bin_edges)-1)])\n", "\n", "data_corr = np.array([405, 958, 1240, 1316, 1161, 1042, 881, 712, 528, 425, 319, \n", "                      250, 183, 142, 104, 77, 55, 42, 25, 23, 15, 12, 9, 4, 4, 1, \n", "                      4, 3, 0, 0, 1, 2, 1, 0, 3])\n", "\n", "MC = np.array([3057, 7241, 9282, 9404, 8559, 7564, 6259, 5124, 3976, 3095, 2445,\n", "       1816, 1398, 1019,  763,  569,  399,  307,  194,  174,  117,   96,\n", "         67,   60,   29,   20,   22,   13,    7,    6,    5,    5,    2,\n", "          3,    3])\n", "\n", "\n", "#YOUR CODE HERE\n", "#Normalize model to data and print the chisquare result\n", "\n", "\n", "plt.bar(bin_centers, MC, width=3.75, color='red') #Plot MC\n", "plt.errorbar(bin_centers,data_corr,yerr=np.sqrt(data_corr),marker='o',linestyle='none') #Plot data\n", "plt.ylabel(\"Number of entries\")\n", "plt.show();\n", "\n", "plt.plot(np.arange(0, 135, .01), np.ones(len(np.arange(0, 135, .01))), color='black')\n", "plt.errorbar(bin_centers, data_corr/MC,yerr=np.sqrt(data_corr)/MC,marker='o',linestyle='none') #Plot ratio of data/MC (should be 1)\n", "plt.ylabel(\"Data/Simulation\")\n", "plt.ylim(0,2)\n", "plt.show();"]}, {"cell_type": "markdown", "id": "48eaed9b", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Problem 5.3.3</span>\n", "\n", "This second $\\chi^2$ test outcome looks much better. However, maybe we can do even better.\n", "\n", "Looking at the ratio plot, we see that most of the data points that have large deviations from the simulation are located in the last few bins. Furthermore, the bulk of the distribution (i.e. the area around the peak) is found in the first 10$-$15 bins.\n", "\n", "Try cutting the data and simulation arrays to include only the first 10 bins and perform the $\\chi^2$ test a third time. What is the new p-value? Enter your answer as a number with precision 1e-3."]}, {"cell_type": "code", "execution_count": null, "id": "b14c84d2", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>PROBLEM: P5.3.3\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "bin_edges = np.array([  0.  ,   3.75,   7.5 ,  11.25,  15.  ,  18.75,  22.5 ,  26.25,\n", "        30.  ,  33.75,  37.5 ,  41.25,  45.  ,  48.75,  52.5 ,  56.25,\n", "        60.  ,  63.75,  67.5 ,  71.25,  75.  ,  78.75,  82.5 ,  86.25,\n", "        90.  ,  93.75,  97.5 , 101.25, 105.  , 108.75, 112.5 , 116.25,\n", "       120.  , 123.75, 127.5 , 131.25])\n", "\n", "bin_centers = np.array([(bin_edges[i]+bin_edges[i+1])/2 for i in range(len(bin_edges)-1)])\n", "\n", "data_corr = np.array([405, 958, 1240, 1316, 1161, 1042, 881, 712, 528, 425, 319, \n", "                      250, 183, 142, 104, 77, 55, 42, 25, 23, 15, 12, 9, 4, 4, 1, \n", "                      4, 3, 0, 0, 1, 2, 1, 0, 3])\n", "\n", "MC = np.array([3057, 7241, 9282, 9404, 8559, 7564, 6259, 5124, 3976, 3095, 2445,\n", "       1816, 1398, 1019,  763,  569,  399,  307,  194,  174,  117,   96,\n", "         67,   60,   29,   20,   22,   13,    7,    6,    5,    5,    2,\n", "          3,    3])\n", "\n", "\n", "\n", "maxbin=10\n", "bin_centers = #INSERT CODE HERE\n", "MC = #INSERT CODE HERE\n", "data_corr = #INSERT CODE HERE\n", "\n", "#YOUR CODE HERE\n", "#Normalize model to data and print the chisquare result\n", "\n", "plt.bar(bin_centers, MC, width=3.75, color='red') #Plot MC\n", "plt.errorbar(bin_centers,data_corr,yerr=np.sqrt(data_corr),marker='o',linestyle='none') #Plot data\n", "plt.ylabel(\"Number of entries\")\n", "plt.show()\n", "\n", "plt.plot(np.arange(0, 3.6*maxbin, .01), np.ones(len(np.arange(0, 3.6*maxbin, .01))), color='black')\n", "plt.errorbar(bin_centers, data_corr/MC,yerr=np.sqrt(data_corr)/MC,marker='o',linestyle='none') #Plot ratio of data/MC (should be 1)\n", "plt.ylabel(\"Data/Simulation\")\n", "plt.ylim(0,2)\n", "plt.show()"]}, {"cell_type": "markdown", "id": "3156991f", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": [">#### Follow-up 5.3.3a (ungraded)\n", ">\n", ">The cut improves the fit between the data and the simulation, but it also cuts out a significant amount of data. In particular, if we are interested in properties like the length of the tail on the high side of the peak, this version of the fit misses that entirely. Play around with the number of bins that are included. At what cut value are you happy with how much data was thrown out vs. fit quality? Can you write a process to automatically select an ideal number of bins based on some fit criteria?"]}, {"cell_type": "markdown", "id": "cfc2e9a1", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_5_4'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">P5.4 A Fitting Example</h2>   \n", "\n", "| [Top](#section_5_0) | [Previous Section](#section_5_3) | [Problems](#problems_5_4) |\n"]}, {"cell_type": "markdown", "id": "53910343", "metadata": {"tags": ["learner", "catsoop_04", "md"]}, "source": ["<h3>Overview</h3>\n", "\n", "To investigate the first step of the minimization process, let's look at a different example.\n", "\n", "Suppose you're an astrophysicist looking at a distant star. Photons hit your telescope at random, independent intervals, so the number that you detect within your period of observation is Poisson distributed.\n", "\n", "Also, this star is really important, and $N\\gg 1$ telescopes are looking at it. Your data $D$ is therefore is $\\{n_1, n_2, \\dots, n_N\\}$, the array of counts observed by all $N$ of the telescopes during one day."]}, {"cell_type": "markdown", "id": "a65ce6b1", "metadata": {"tags": ["learner", "catsoop_04", "md"]}, "source": ["Let's generate some Poisson-distributed sample data for each telescope. We'll assume a parameter of $\\lambda=5$ counts per day, with $N=100$ telescopes."]}, {"cell_type": "code", "execution_count": null, "id": "b2f1cafd", "metadata": {"tags": ["learner", "py", "catsoop_04", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: P5.4-runcell01\n", "\n", "import numpy as np\n", "np.random.seed(15)\n", "\n", "LAMBDA = 5\n", "N = 100\n", "\n", "counts = np.random.poisson(LAMBDA, N);\n", "#Optionally print the counts\n", "#print(counts)\n", "\n", "#Look at the plot compared to the true function\n", "bins = np.arange(np.max(counts)+2)-0.5\n", "xs = bins[:-1]+0.5\n", "y_hist_vals, binEdges = np.histogram(counts, bins=bins)\n", "plt.hist(counts, bins=bins)\n", "plt.plot(xs, N * poisson.pmf(xs,LAMBDA), label=\"True distro\", color=\"C1\", linewidth=2)\n", "plt.legend(loc=1)"]}, {"cell_type": "markdown", "id": "937c8a06", "metadata": {"tags": ["learner", "catsoop_04", "md"]}, "source": ["Since each telescope's detection $n_i$ is independent, the probability of detecting data set $D$ given some estimate of $\\lambda$, is simply the product of the probability for each telescope to detect $n_i$. This probability is Poisson distributed.\n", "\n", "We would like to use the `lmfit` minimization function, which does not actually request the likelihood as an input. Instead, it asks for the logarithm of the likelihood of each data point, and assumes that each data point is independent. Internally, it adds all the likelihoods from all data points to get the log-likelihood of the data.\n", "\n", "Run the cell below to define the `log_likelihood`."]}, {"cell_type": "code", "execution_count": null, "id": "8ea4a5ec", "metadata": {"tags": ["learner", "py", "catsoop_04", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: P5.4-runcell02\n", "\n", "import numpy as np\n", "from scipy.stats import poisson\n", "\n", "def log_likelihood(l, data):\n", "    return np.log(poisson.pmf(data, l))"]}, {"cell_type": "markdown", "id": "a7d71abd", "metadata": {"tags": ["learner", "catsoop_04", "md"]}, "source": ["Use the `lmfit`'s minimization function to maximize the likelihood (i.e. minimize the negative likelihood) to see if it finds the true value $\\lambda=5$."]}, {"cell_type": "code", "execution_count": null, "id": "953ff848", "metadata": {"tags": ["learner", "py", "catsoop_04", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: P5.4-runcell03\n", "\n", "from lmfit import Parameters, minimize, fit_report\n", "\n", "def negative_log_likelihood(l, data):\n", "    return -log_likelihood(l, data)\n", "\n", "params = Parameters()\n", "params.add('l', min=0, value=1)\n", "\n", "result = minimize(negative_log_likelihood, params, args=(counts,))\n", "print(fit_report(result))"]}, {"cell_type": "markdown", "id": "f92542ac", "metadata": {"tags": ["learner", "catsoop_04", "md"]}, "source": ["You see that the minimization result is not exactly $\\lambda=5$, but is equal to $5$ within the fit uncertainty. To further evaluate the fit quality, let's plot the data, true, and best fit distributions."]}, {"cell_type": "code", "execution_count": null, "id": "ec1a77d2", "metadata": {"tags": ["learner", "py", "catsoop_04", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: P5.4-runcell04\n", "\n", "import matplotlib.pyplot as plt\n", "\n", "bins = np.arange(np.max(counts)+2)-0.5\n", "xs = bins[:-1]+0.5\n", "plt.hist(counts, bins=bins, label=\"Photon data\", fill=False, histtype=\"step\", color='k', linewidth=2)\n", "\n", "y, binEdges = np.histogram(counts, bins=bins)\n", "bincenters = 0.5*(binEdges[1:]+binEdges[:-1])\n", "plt.errorbar(bincenters,y,yerr=np.sqrt(y),marker='o',color='k', linestyle='none')\n", "\n", "plt.plot(xs, N * poisson.pmf(xs, result.params['l'].value), label=\"Best fit distro\", color=\"C0\", linewidth=2)\n", "plt.plot(xs, N * poisson.pmf(xs,LAMBDA), label=\"True distro\", color=\"C1\", linewidth=2)\n", "\n", "plt.xlabel(\"Number of photons observed\")\n", "plt.ylabel(\"Number of telescopes\")\n", "plt.legend()\n", "\n", "pred_y = (N) * poisson.pmf(bincenters, result.params['l'].value)\n", "pred_y = pred_y/np.sum(pred_y) * np.sum(y)\n", "\n", "chi2,p = chisquare(y, pred_y)\n", "print(\"Chi-squared:\",chi2)\n", "print(\"Probability: {:.4%}\".format(p)) "]}, {"cell_type": "markdown", "id": "3f57ef22", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='problems_5_4'></a>   \n", "\n", "| [Top](#section_5_0) | [Restart Section](#section_5_4) |\n"]}, {"cell_type": "markdown", "id": "002a2231", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Problem 5.4.1</span>\n", "\n", "To represent the uncertainty on $\\lambda$ in a graphical way, we will plot uncertainty bounds on our histogram. In particular, we will do the following:\n", "\n", "- compute the Poisson distribution for 5 values of $\\lambda$ in the range $[\\lambda - 2\\sigma_\\lambda$, $\\lambda+2\\sigma_\\lambda$], where $\\sigma_\\lambda$ is the uncertainty on $\\lambda$ generated by the fit, which you can get with `result.params['l'].stderr`\n", "- for each bin, obtain the minimum and maximum Poisson predicted values from the 5 distributions\n", "- use `plt.fill_between` to shade the area between the min and max values in each bin.\n", "\n", "Ultimately, we want the lower edge of the error band in each bin to represent the lowest Poisson predicted value among all 5 distributions computed above. Similarly, the higher edge should represent the highest Poisson predicted value among the 5 distributions.\n", "\n", "Having completed the task above, consider this question: **If 10 telescopes report observing 6 photons, is this consistent with the value of $\\lambda$ that you found from your fit procedure, within $2\\sigma$?**"]}, {"cell_type": "code", "execution_count": null, "id": "8c99d014", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>PROBLEM: P5.4.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "from scipy.stats import poisson\n", "\n", "bins = np.arange(np.max(counts)+2)-0.5\n", "xs = bins[:-1]+0.5\n", "plt.hist(counts, bins=bins, label=\"Photon data\", fill=False, histtype=\"step\", color='k', linewidth=2)\n", "\n", "y,binEdges = np.histogram(counts, bins=bins)\n", "bincenters = 0.5*(binEdges[1:]+binEdges[:-1])\n", "plt.errorbar(bincenters,y,yerr=np.sqrt(y),marker='o',color='k', linestyle='none')\n", "\n", "plt.plot(xs, N * poisson.pmf(xs, result.params['l'].value), label=\"Best fit distro\", color=\"C0\", linewidth=2)\n", "plt.plot(xs, N * poisson.pmf(xs,LAMBDA), label=\"True distro\", color=\"C1\", linewidth=2)\n", "\n", "####################\n", "# Insert Code Here #\n", "####################\n", "\n", "minimum = None # Placeholder Value - Fill in the correct line\n", "maximum = None # Placeholder Value - Fill in the correct line\n", "\n", "####################\n", "\n", "plt.xlabel(\"Number of photons observed\")\n", "plt.ylabel(\"Number of telescopes\")\n", "plt.legend()\n", "plt.show();"]}, {"cell_type": "markdown", "id": "7ee6d50c", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Problem 5.4.2</span>\n", "\n", "For some number of photon counts in the range `[0,10]`, find a number of telescope observations that falls below the $2\\sigma$ bound outlined in the previous problem. Report your numbers as integers in the following format: `[number of photons, number of telescopes]`."]}, {"cell_type": "markdown", "id": "9f3a3553", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": [">#### Follow-up 5.4.2a (ungraded)\n", ">\n", ">Suppose one of our telescopes also records information about the energy of each photon from the star. Over time, we could produce a histogram of these values representing the spectrum of the star's light output. Suppose we also have a model of the star that predicts a certain spectrum, given some parameters. How could we make a best estimate of these parameters (assuming systematic uncertainties are small relative to statistical uncertainties)?\n"]}], "metadata": {"celltoolbar": "Tags", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}