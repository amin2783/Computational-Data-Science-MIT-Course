{"cells": [{"cell_type": "markdown", "id": "168bf389", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<hr style=\"height: 1px;\">\n", "<i>This notebook was authored by the 8.S50x Course Team, Copyright 2022 MIT All Rights Reserved.</i>\n", "<hr style=\"height: 1px;\">\n", "<br>\n", "\n", "<h1>Guided Problem Set 2: Error Propagation</h1>\n"]}, {"cell_type": "markdown", "id": "6d7e33ca", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_2_0'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">P2.0 Overview</h2>\n"]}, {"cell_type": "markdown", "id": "3d97b41e", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<h3>Navigation</h3>\n", "\n", "<table style=\"width:100%\">\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_2_1\">P2.1 Error Propagation - A Simple Example</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#problems_2_1\">P2.1 Problems</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_2_2\">P2.2 Error Propagation - A More Complicated Example</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#problems_2_2\">P2.2 Problems</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_2_3\">P2.3 Johnson Noise</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#problems_2_3\">P2.3 Problems</a></td>\n", "    </tr>\n", "</table>"]}, {"cell_type": "markdown", "id": "f99ed233", "metadata": {"tags": ["learner", "catsoop_00", "md"]}, "source": ["<h3>Motivation</h3>\n", "\n", "When taking measurements, we often indirectly perform measurements of fundamental parameters, by taking a measurement on as specific phenomenon and then using our knowledge of physics to translate this into a meaningful physics measurement. In this problem set we go through the details of how to propagate errors into formulas and extract the uncertainty on the result of the formula. \n", "\n", "\n", "<h3>Learning Objectives</h3>\n", "\n", "In this problem set we will explore the following topics:\n", "\n", "- Introduction to error propagation formula and its application\n", "- Basic error propagation through Gaussian distributions\n", "- Application of error propagation to assess upper and lower bounds\n", "- Introduction to use of statistical and systematical errors\n", "- Application of error propagation to physics formulas"]}, {"cell_type": "markdown", "id": "2f808567", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<h3>New Library</h3>\n", "\n", "**Using Jupyter Notebook Locally**\n", "\n", "Starting in this section of the course, you will need access to the `lmfit` library. If you are running Juypter locally, you don't need to do anything, as this library was installed during the initial setup of your 8.S50x environment. So, you can ignore the first code cell below and jump right to the import code.\n", "\n", "If you didn't perform this installation (or others), then activate your 8.S50x conda environment and execute the following installations:\n", "\n", "<pre>\n", "conda install lmfit\n", "</pre>\n", "\n", "\n", "**Using Colab**\n", "\n", "However, if you are running this notebook in a Colab environment, the procedure is slightly different. Unlike the libraries that were used previously, `lmfit` is not included in the default Colab environment. To do this installation, you must run the `!pip install lmfit` command in the code cell below."]}, {"cell_type": "code", "execution_count": null, "id": "2c01cf77", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: P2.0-runcell00\n", "\n", "!pip3 install lmfit "]}, {"cell_type": "markdown", "id": "8cc1a52f", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Note on Installing Libraries in Colab</h3>\n", "\n", "The `lmfit ` installation will only exist for the duration of your current Colab \"runtime\". Closing a notebook, or even quitting your browser, will not cause the runtime to end right away. However, if you don't do anything for a while it will eventually stop. When it does that, you will need to reinstall `lmfit` for your new runtime.\n", "\n", "When opening any notebook, you can tell if you still have an active runtime by looking at the upper right of the Colab window. If you see two bars labeled \"RAM\" and \"DISK\", you don't need to reinstall anything. If, instead, you see a pull-down menu labeled \"Connect\", you are starting a new runtime and will need to redo any required installations."]}, {"cell_type": "markdown", "id": "e9bec096", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Importing Libraries</h3>\n", "\n", "Before beginning, run the cell below to import the relevant libraries for this notebook."]}, {"cell_type": "code", "execution_count": null, "id": "ea182157", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: P2.0-runcell01\n", "\n", "import numpy as np                 #https://numpy.org/doc/stable/\n", "import matplotlib.pyplot as plt    #https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.html\n", "import scipy.stats                 #https://docs.scipy.org/doc/scipy/reference/stats.html\n", "from scipy.integrate import trapz  #https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.integrate.trapz.html\n", "from lmfit.models import Model     #https://lmfit.github.io/lmfit-py/model.html"]}, {"cell_type": "markdown", "id": "60d7833e", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Setting Default Figure Parameters</h3>\n", "\n", "The following code cell sets default values for figure parameters."]}, {"cell_type": "code", "execution_count": null, "id": "783e9667", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: P2.0-runcell02\n", "\n", "#set plot resolution\n", "%config InlineBackend.figure_format = 'retina'\n", "\n", "#set default figure parameters\n", "plt.rcParams['figure.figsize'] = (9,6)\n", "\n", "medium_size = 12\n", "large_size = 15\n", "\n", "plt.rc('font', size=medium_size)          # default text sizes\n", "plt.rc('xtick', labelsize=medium_size)    # xtick labels\n", "plt.rc('ytick', labelsize=medium_size)    # ytick labels\n", "plt.rc('legend', fontsize=medium_size)    # legend\n", "plt.rc('axes', titlesize=large_size)      # axes title\n", "plt.rc('axes', labelsize=large_size)      # x and y labels\n", "plt.rc('figure', titlesize=large_size)    # figure title"]}, {"cell_type": "markdown", "id": "118b0074", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_2_1'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">P2.1 Error Propagation - A Simple Example</h2>    \n", "\n", "| [Top](#section_2_0) | [Previous Section](#section_2_0) | [Problems](#problems_2_1) | [Next Section](#section_2_2) |\n"]}, {"cell_type": "markdown", "id": "4b894b9b", "metadata": {"tags": ["learner", "catsoop_01", "md"]}, "source": ["<h3>Motivation: Finding Uncertainty of a Calculated Quantity</h3>\n", "\n", "- We often encounter functions that depend on a number of measured quantities, each with its associated independent Gaussian errors\n", "- To understand the uncertainty in the calculated quantity, we need to determine how the uncertainties in the individual quantities propagate through the function\n", "- Often these functions can be treated with standard error propagration: $ \\Delta f(\\{x_i\\}) = \\sqrt{\\sum_i (\\partial f / \\partial x_i)^2 (\\Delta x_i)^2} $\n", "- Sometimes things get complicated. Good physicists are (somewhat) lazy so they (sometimes) use NumPy to help propagate errors through more complex functions\n"]}, {"cell_type": "markdown", "id": "dd9071a5", "metadata": {"tags": ["learner", "catsoop_01", "md"]}, "source": ["<h3>A (Very) Simple Example</h3>\n", "\n", "Suppose we have data points ($x,y$) with independent Gaussian errors $\\Delta x$ and $\\Delta y$. We want to compute $f(x,y) = x + y$ and its error.\n", "\n", "We can use our error propagation formula above, which we can write out in its two variable form as\n", "\n", "$$\\Delta f(x,y) = \\sqrt{(\\partial f / \\partial x)^2(\\Delta x)^2 + (\\partial f / \\partial y)^2(\\Delta y)^2}$$\n", "\n", "Computing the derivatives, we get \n", "\n", "$$\\Delta f = \\sqrt{(\\Delta x)^2 + (\\Delta y)^2} $$"]}, {"cell_type": "code", "execution_count": null, "id": "c868f597", "metadata": {"tags": ["learner", "py", "catsoop_01", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: P2.1-runcell01\n", "\n", "def f(x, y):\n", "    return x + y\n", "\n", "def delta_f(delta_x, delta_y):\n", "    return np.sqrt((delta_x**2.)+(delta_y**2.))\n", "\n", "x_val = 5.\n", "x_err = 2.\n", "\n", "y_val = 9.\n", "y_err = 2.\n", "\n", "print(\"f(x) = %f +/- %f\" % (f(x_val, y_val), delta_f(x_err, y_err)))\n"]}, {"cell_type": "markdown", "id": "46756a13", "metadata": {"tags": ["learner", "catsoop_01", "md"]}, "source": ["<br>\n", "\n", "Now let's try using NumPy to run actual Gaussian distributions through this function. We will sample from two normal distributions with means and standard deviations defined above, spefically:\n", "\n", "<pre>\n", "#x: mean, std\n", "x_val = 5.\n", "x_err = 2.\n", "\n", "#y: mean, std\n", "y_val = 9.\n", "y_err = 2.\n", "</pre>\n", "\n", "We simply take the sum of the samples from these distributions, `f_samples`, and compute the mean and standard deviation of this data set, `(np.mean(f_samples),np.std(f_samples))`. We then compare to the expected values, defined by the function above.\n", "\n", "Ultimately, we find that the mean and error from the sampled data reflect our expectation based on the definition of propagation of error, above. Indeed, try larger sample size to see this convincingly: `N_SAMPLES = 1000000`"]}, {"cell_type": "code", "execution_count": null, "id": "ee9f6444", "metadata": {"tags": ["learner", "py", "catsoop_01", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: P2.1-runcell02\n", "\n", "np.random.seed(2)\n", "N_SAMPLES = 10000\n", "N_BINS = 100\n", "x_samples = np.random.normal(loc = x_val, scale = x_err, size = N_SAMPLES)\n", "y_samples = np.random.normal(loc = y_val, scale = y_err, size = N_SAMPLES)\n", "\n", "f_samples = f(x_samples, y_samples)\n", "\n", "print(\"observed: f(x) = %f +/- %f\" % (np.mean(f_samples), np.std(f_samples)))\n", "print(\"expected: f(x) = %f +/- %f\" % (f(x_val, y_val), delta_f(x_err, y_err)))\n", "\n", "#MAKING A PLOT\n", "counts, bin_edges = np.histogram(f_samples, bins = N_BINS, density = True)\n", "bin_centers = 0.5*(bin_edges[:-1]+bin_edges[1:])\n", "\n", "#Plotting the data\n", "#Alternatively use: #plt.plot(bin_centers,counts)\n", "plt.step(bin_edges[1:],counts)\n", "\n", "#Plotting Gaussian with mean and std given by  f and delta_f\n", "plt.plot(bin_centers, scipy.stats.norm.pdf(bin_centers, loc = f(x_val, y_val), scale = delta_f(x_err, y_err)))"]}, {"cell_type": "markdown", "id": "4b791c59", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='problems_2_1'></a>     \n", "\n", "| [Top](#section_2_0) | [Restart Section](#section_2_1) | [Next Section](#section_2_2) |\n"]}, {"cell_type": "markdown", "id": "1492d4f0", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Problem 2.1.1</span>\n", "\n", "Let $g(x_1,x_2,x_3...x_n) = x_1 + x_2 + x_3 + ... + x_n$ be a function of $n$ variables, where each variable has an error of 5. Complete the code below to manually compute the error of $g$ with 2 variables, 101 variables, and 5012 variables. \n", "\n", "\n", "Hint: Begin with the definition for error propagation. What does it reduce to when the `n` errors are the same?\n", "\n", "$$\\Delta g(x_1, x_2, x_3...x_n) = \\sqrt{(\\partial g / \\partial x_1)^2(\\Delta x_1)^2 + (\\partial g / \\partial x_2)^2(\\Delta x_2)^2 +...+(\\partial g / \\partial x_1)^2(\\Delta x_1)^2}$$"]}, {"cell_type": "code", "execution_count": null, "id": "fec58084", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>PROBLEM: P2.1.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def delta_g(delta_x, n):\n", "  ####################\n", "  # Insert Code Here #\n", "  ####################\n", "  return \n", "\n", "x_err = 5.\n", "\n", "n=2\n", "print(\"g_err(n=2) = %f\" % (delta_g(x_err, n)))\n", "n=101\n", "print(\"g_err(n=101) = %f\" % (delta_g(x_err, n)))\n", "n=5012\n", "print(\"g_err(n=5012) = %f\" % (delta_g(x_err, n)))\n"]}, {"cell_type": "markdown", "id": "f764e4db", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": [">#### Follow-up 2.1.1a (ungraded)\n", ">   \n", ">Why is it important to minimize the number of variables that contribute to your uncertainty? A lot of functions in physics analysis are dependent on not just a linear combination of terms, but perhaps an exponential one. What would the error of this look like with $n$ variables?\n"]}, {"cell_type": "markdown", "id": "9f1952f3", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_2_2'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">P2.2 Error Propagation - A More Complicated Example</h2>    \n", "\n", "| [Top](#section_2_0) | [Previous Section](#section_2_1) | [Problems](#problems_2_2) | [Next Section](#section_2_3) |\n"]}, {"cell_type": "markdown", "id": "d4d962c5", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='problems_2_2'></a>    \n", "\n", "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Problem 2.2.1</span>\n", "\n", "Now let's take $h(x,y) = (\\sqrt{|x|} + \\sqrt{|y|})\\cdot (x - y)$\n", "\n", "Fill in the following code cell to compute the error on `h(x,y)` using the same values of $x$ and $y$ (and their respective errors) from the example code in P2.1. Take the stdev of `h(x,y)` and add it to the mean of `h(x,y)` to get an upper bound that is \"one error away\". What is this value?\n", "\n", "Enter your answer as a number with precision 1e-1.\n", "\n", "**NOTE:** You must specifically use the random seed and number of samples defined in the code below, for comparison with the answer checker."]}, {"cell_type": "code", "execution_count": null, "id": "c33cba14", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>PROBLEM: P2.2.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "np.random.seed(2)\n", "N_SAMPLES = 100000\n", "N_BINS = 100\n", "x_val = 5.\n", "x_err = 2.\n", "y_val = 9.\n", "y_err = 2.\n", "x_samples = np.random.normal(loc = x_val, scale = x_err, size = N_SAMPLES)\n", "y_samples = np.random.normal(loc = y_val, scale = y_err, size = N_SAMPLES)\n", "\n", "def h(x,y):\n", "    return (np.sqrt(np.abs(x))+np.sqrt(np.abs(y)))*(x-y)\n", "\n", "\n", "####################\n", "# Insert Code Here #\n", "####################\n", "\n", "def h_val_err(ix, iy):\n", "    h_samples = None # Placeholder Value - Fill in the correct line\n", "    h_val = None # Placeholder Value - Fill in the correct line\n", "    h_err = None # Placeholder Value - Fill in the correct line\n", "    return h_val, h_err\n", "\n", "\n", "####################\n", "\n", "\n", "h_val, h_err = h_val_err(x_samples, y_samples)\n", "\n", "print(\"observed: h(x) upper bound\", h_val + h_err)"]}, {"cell_type": "markdown", "id": "1bbfe7a7", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Problem 2.2.2</span>\n", "\n", "Write a function that computes the error in `h(x,y)`. What is the expected error, based on the propagation of error formula? Does this match the error that you obtained in the previous problem?\n", "\n", "\n", "Use the values given previously to evaluate `delta_h(x_val,y_val,delta_x,delta_y)`:\n", "\n", "<pre>\n", "x_val = 5.\n", "x_err = 2.\n", "y_val = 9.\n", "y_err = 2.\n", "</pre>\n", "\n", "Enter your answer as a number with precision 1e-2."]}, {"cell_type": "code", "execution_count": null, "id": "b26e293f", "metadata": {"scrolled": false, "tags": ["py", "draft", "learner_chopped"]}, "outputs": [], "source": ["#>>>PROBLEM: P2.2.2\n", "\n", "#defining delta_h\n", "####################\n", "\n", "def delta_h(x_val,y_val,delta_x,delta_y):\n", "    x_deriv = 0 #YOUR CODE HERE\n", "    y_deriv = 0 #YOUR CODE HERE\n", "    return np.sqrt(x_deriv**2. * delta_x**2. + y_deriv**2. * delta_y**2.)\n", "\n", "\n", "print(\"observed: h(x) = %f +/- %f\" % (h_val, h_err))\n", "print(\"expected: h(x) = %f +/- %f\" % (h(x_val, y_val), delta_h(x_val,y_val,x_err,y_err)))"]}, {"cell_type": "markdown", "id": "d297a029", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": [">#### Follow-up 2.2.2a (ungraded)\n", ">   \n", ">Approximate the data as a Gaussian with mean and variance. How well does this Gaussian compare with the data? How does this Gaussian depend on the mean and variance of the underlying distributions?\n"]}, {"cell_type": "markdown", "id": "67f17f68", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_2_3'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">P2.3 Johnson Noise</h2>   \n", "\n", "| [Top](#section_2_0) | [Previous Section](#section_2_2) | [Problems](#problems_2_3) |\n"]}, {"cell_type": "markdown", "id": "40a6abbf", "metadata": {"tags": ["learner", "catsoop_03", "md"]}, "source": ["<h3>Overview</h3>\n", "\n", "Let's apply this to something slightly more useful, such as measuring the Boltzmann constant, $k_B$, and it's associated error. We outline the scheme as follows:\n", "\n", "- $k_B$ is related to four variables that we can measure with uncertainties (explained further below)\n", "- the relation between $k_B$ and these variables is complicated, so propagating error by using an analytic equation is not possible\n", "- we will effectively calculate $k_B$ multiple times using random samples from the data, then determine the uncertainty from the multiple calculations\n", "\n", "**Johnson Noise**\n", "\n", "To calculate $k_B$, we will use measurments of the Johnson noise, which is the thermal noise across a resistor $V^2/4 T$, defined by:\n", "\n", "$$\\frac{V^2}{4 T} = k_B R\\int_{0}^{\\infty} \\frac{g(f)^2}{1+ (2\\pi R C f)^2}df$$\n", "\n", "The variables in the preceding equation are quantites measured from an experiment:\n", "\n", "<pre>\n", "f = frequency\n", "g = gain\n", "R = resistance\n", "C = capacitance\n", "</pre>\n", "\n", "We will first compute the total uncertainty on this complicated quantity.\n", "\n", "**Data**\n", "\n", "Suppose we measured the gain, $g$, as a function of frequency, $f$, and we have some uncertainties on $R$ and $C$. The \"data\" are given below. Run the cell to assign these values."]}, {"cell_type": "code", "execution_count": null, "id": "5dbd4019", "metadata": {"tags": ["learner", "py", "catsoop_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: P2.3-runcell01\n", "\n", "frequency = np.array([   200.,    300.,    400.,    500.,    600.,    700.,    800.,\n", "          900.,   1000.,   1100.,   1200.,   1300.,   1400.,   1500.,\n", "         1700.,   2000.,   3000.,   4000.,   5000.,   7000.,  10000.,\n", "        13000.,  15000.,  17000.,  20000.,  25000.,  30000.,  35000.,\n", "        40000.,  45000.,  50000.,  55000.,  60000.,  65000.,  70000.,\n", "        75000.,  80000.,  85000.,  90000.,  95000., 100000.])\n", "\n", "gain = np.array([  1.56572199,   7.56008454,  24.23507344,  58.36646477,\n", "       119.11924863, 215.75587662, 354.79343025, 517.34083494,\n", "       679.81395988, 805.18954729, 877.53623188, 944.14612835,\n", "       951.12203586, 981.66551215, 976.08071562, 971.57565072,\n", "       991.33195051, 974.54482165, 968.02100388, 970.96127868,\n", "       972.70192708, 980.9122768 , 983.62597547, 981.85446382,\n", "       964.75994752, 984.27991886, 959.44478862, 975.87335094,\n", "       906.24841379, 831.8699187 , 695.5940221 , 562.69096627,\n", "       426.50959034, 328.93671408, 248.14630158, 198.16023325,\n", "       150.59357167, 121.00349255, 100.86777721,  79.42663031,\n", "        63.20952534])\n", "\n", "gain_uncertainty = np.array([5.21317443e-03, 3.11522352e-02, 1.17453781e-01, 1.54063502e-01,\n", "       1.27335068e+00, 1.27124575e+00, 1.62862522e+00, 8.07632112e-01,\n", "       1.39800408e+00, 1.52872753e+00, 9.26100943e-01, 2.07700290e+00,\n", "       2.41624111e+00, 2.48737608e+00, 2.66446131e+00, 6.30956544e+00,\n", "       2.48543922e+00, 5.85031911e+00, 5.36245736e+00, 5.03316166e+00,\n", "       5.96042863e+00, 1.80119083e+00, 2.19189309e+00, 4.76416499e+00,\n", "       2.60518705e+00, 8.91016625e-01, 8.68517783e-01, 7.60893395e-02,\n", "       1.12595429e+00, 9.59211786e-01, 2.11207039e+00, 1.54206027e+00,\n", "       6.15658573e-01, 2.21068956e+00, 1.93131996e+00, 1.17159272e+00,\n", "       1.02084395e+00, 6.45939329e-01, 1.15822783e+00, 1.50426555e-01,\n", "       2.64213908e-01])\n", "\n", "resistance = np.array([477.1e3, 810e3, 99.7e3, 502.3e3, 10.03e3]) \n", "resistance_uncertainty = np.array([0.2e3, 2e3, 0.2e3, 0.3e3, 0.3e3])\n", "\n", "capacitance = 125e-12\n", "capacitance_uncertainty = 14e-12\n", "\n", "#measured Johnson Noise and uncertainty\n", "#note that this is the value of the left side of the equation shown above\n", "v2rmsd4t = np.array([2.57337556e-08, 1.96214066e-08, 2.21758082e-08, 2.38320749e-08,\n", "       7.31633110e-09])\n", "v2rmsd4t_uncertainty = np.array([1.25267830e-09, 1.46644504e-09, 1.08426579e-09, 1.77538860e-09,\n", "       2.07583938e-10])\n"]}, {"cell_type": "markdown", "id": "aefb366e", "metadata": {"tags": ["learner", "catsoop_03", "md"]}, "source": ["Now we'll create a function `mc_compute` which uses some fun python tricks to compute the full term $R\\int_{0}^{\\infty} \\frac{g(f)^2}{1+ (2\\pi R C f)^2}df$ of some random sample (based on normal distributions), given the necessary input variables."]}, {"cell_type": "code", "execution_count": null, "id": "e0bdb35a", "metadata": {"tags": ["learner", "py", "catsoop_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: P2.3-runcell02\n", "\n", "from scipy.integrate import trapz\n", "\n", "def mc_compute(freq, gain, gain_error, r, rerr, cap, cap_err, n_samp):\n", "    samples = []\n", "    for k in range(n_samp):\n", "        mc_gain = gain + np.random.normal(len(gain))*gain_error\n", "        mc_r = r + rerr*np.random.normal(1)\n", "        mc_cap = cap + cap_err*np.random.normal(1)\n", "        mc_integrand = mc_gain**2.0/(1+ (2*np.pi*mc_r*mc_cap*freq)**2.0)\n", "        mc_int = scipy.integrate.trapz(mc_integrand, freq)\n", "        samples.append(mc_r*mc_int)\n", "    return np.array(samples)"]}, {"cell_type": "markdown", "id": "eb295499", "metadata": {"tags": ["learner", "catsoop_03", "md"]}, "source": ["Next, we'll compute the above formula for multiple samples, taken at different resistance, which is stored in the `rgr` array, assigned below.\n", "\n", "To be clear, `rgr` is an array of elements `rgr_i`, where the ith element is calculated using `mc_compute`, such that:\n", "\n", "$$\\mathrm{rgr}_i = R_i\\left[\\int_{0}^{\\infty} \\frac{g(f)^2}{1+ (2\\pi R_i C f)^2}df\\right]_i$$\n", "\n", "\n", "Notice below how we also keep track of the error in `rgr_unc`. Once again, python proves to be very helpful!"]}, {"cell_type": "code", "execution_count": null, "id": "e329e0ce", "metadata": {"tags": ["learner", "py", "catsoop_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: P2.3-runcell03\n", "\n", "rgr = []\n", "rgr_unc = []\n", "for k in range(5):\n", "    samples = mc_compute(frequency, gain, gain_uncertainty, resistance[k], resistance_uncertainty[k], capacitance, capacitance_uncertainty,100)\n", "    rgr.append(np.mean(samples))\n", "    rgr_unc.append(np.std(samples))\n", "rgr = np.array(rgr)  \n", "rgr_unc = np.array(rgr_unc)"]}, {"cell_type": "markdown", "id": "c51d051e", "metadata": {"tags": ["learner", "catsoop_03", "md"]}, "source": ["We then plot the measured Johnson noise $V^2/4T$ (y-axis) versus `rgr_i` (x-axis). From the formula given above, this should give a straight line with a slope of $k_B$, the Boltzmann constant we are trying to find."]}, {"cell_type": "code", "execution_count": null, "id": "bd7062a1", "metadata": {"tags": ["learner", "py", "catsoop_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: P2.3-runcell04\n", "\n", "plt.errorbar(rgr, v2rmsd4t, yerr = v2rmsd4t_uncertainty, xerr= rgr_unc, fmt = 'o' )\n", "plt.show()"]}, {"cell_type": "markdown", "id": "58f27c1f", "metadata": {"tags": ["learner", "catsoop_03", "md"]}, "source": ["Again, it's important to notice that we obtained the uncertainty pertaining to `rgr`, the Johnson noise uncertainty, purely from its measurement. Both of these sources of uncertainty are integral to calculating the error on the Boltzmann constant.\n", "\n", "Notice that one of the points has a very small measured Johnson noise value with a correspondingly small uncertainty and the value calculated from the integral also has a small uncertainty. As a result, the error bars on this point are too small to be visible"]}, {"cell_type": "markdown", "id": "8aff2ddc", "metadata": {"tags": ["learner", "catsoop_03", "md"]}, "source": ["**Using lmfit to find the fit parameters and uncertainty**\n", "\n", "Lastly, for fun (we will discuss this in future lectures) we use `lmfit`'s linear fit to find the Boltzmann constant (called \"k\" in the code below) and its error. \n", "\n", "Note that to avoid problems with very large or very small numbers in the fit, the various quantities are rescaled before calling `lmod.fit`."]}, {"cell_type": "code", "execution_count": null, "id": "54efaf35", "metadata": {"tags": ["learner", "py", "catsoop_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: P2.3-runcell05\n", "\n", "from lmfit.models import Model\n", "\n", "def linear_model(x, k, b):\n", "    return x/k+b\n", "\n", "lmod = Model(linear_model)\n", "lmod.set_param_hint(name = 'k', value = 1)\n", "lmod.set_param_hint(name = 'b', value = 1)\n", "result = lmod.fit(1e-15*rgr, x = v2rmsd4t*1e8, weights = 1/(rgr_unc*1e-15))\n", "print(result.fit_report())\n", "result.plot_fit()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "4a18ed9e", "metadata": {"tags": ["learner", "catsoop_03", "md"]}, "source": ["This is pretty close! The Boltzmann constant is actually $1.380649 \u00d7 10^{-23} (\\mathrm{m}^{2} \\mathrm{kg})/ (\\mathrm{s}^{2} \\mathrm{K})$.\n", "\n", "Note, the model that is fit adds a non-zero intercept, but the fitted value of `b` is consistent with zero within uncertainties."]}, {"cell_type": "markdown", "id": "a65729e8", "metadata": {"tags": ["learner", "catsoop_03", "md"]}, "source": ["<h3>Pendulum Example</h3>\n", "\n", "Using an approach similar to the Johnson noise example, we will now calculate the error on Newton's gravitational constant given some experimental data. The setup is as follows.\n", "\n", "Suppose you were asked to determine Newton's gravitational constant, $G$. To do this, you set up a pendulum. We know from the equation of a pendulum, that the period of oscillation is given by\n", "\n", "$$T = 2 \\pi \\sqrt{\\frac{l}{g}}$$\n", "\n", "where the gravitational constant at the surface of the Earth $g$ is given by:\n", "\n", "\n", "$$g = G\\frac{M_{earth}}{r_{earth}^2}$$\n", "\n", "In the experiment, the lengths of the pendulum are varied and the quantity $\\left(2 \\pi/T\\right)^2$ is measured. $M_{earth}$ and $r_{earth}$ are known, with some uncertainty.\n", "\n", "The constants, data, and uncertainties are all defined below. Please run the cell."]}, {"cell_type": "code", "execution_count": null, "id": "bf612f99", "metadata": {"tags": ["learner", "py", "learner_chopped", "catsoop_03"]}, "outputs": [], "source": ["#>>>RUN: P2.3-runcell06\n", "\n", "m_earth = 5.9722 * 10**24 \n", "m_earth_unc = 6*10^22\n", "\n", "r_earth = 6.371 * 10**6\n", "r_earth_unc = 100000\n", "\n", "#length and unc\n", "l = [.1, .2, .3, .4, .5, .6, .7, .8, .9, 1.0]\n", "l_unc = [.01, .02, .03, .03, .03, .04, .03, .04, .05, .04]\n", "\n", "T = [0.6468, 0.9317, 1.1352, 1.4553, 1.6181, 1.782,  1.969,  2.068,  2.211,  2.255 ]\n", "#Note, we are assuming that the experiment is run for a very large number of pendulum swings\n", "#so that the uncertainty in the period is negligible\n", "\n", "\n", "pi2dTsqrd = ((2*np.pi)/np.array(T))**2\n", "pi2dTsqrd_unc = [.15, .15, .2, .1, .1, .2, .2, .2, .2, .2]"]}, {"cell_type": "markdown", "id": "985ec672", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='problems_2_3'></a>   \n", "\n", "| [Top](#section_2_0) | [Restart Section](#section_2_3) |\n"]}, {"cell_type": "markdown", "id": "3df42c28", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Problem 2.3.1</span>\n", "\n", "Using an approach similar to the Johnson noise example, calculate the error on Newton's gravitational constant in this experiment as a percent. Enter your answer as a percentage with precision 1e-3 (for instance an answer of 7.25% would be entered as 7.250). Use the code below as a starting point.\n"]}, {"cell_type": "code", "execution_count": null, "id": "3f754b30", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>PROBLEM: P2.3.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "np.random.seed(2)\n", "from lmfit.models import Model\n", "\n", "def compute(m_earth, m_earth_unc, r_earth, r_earth_unc, l, l_unc, n_samp):\n", "    samples = []\n", "    ########################\n", "    ### INSERT CODE HERE ###\n", "    ########################\n", "    return np.array(samples)\n", "\n", "\n", "def get_rgr_data(m_earth, m_earth_unc, r_earth, r_earth_unc, l, l_unc, n_samp):\n", "    rgr = []\n", "    rgr_unc = []\n", "    for k in range(10):\n", "        ########################\n", "        ### INSERT CODE HERE ###\n", "        ########################\n", "        \n", "    rgr = np.array(rgr)  \n", "    rgr_unc = np.array(rgr_unc)\n", "    return rgr, rgr_unc\n", "\n", "\n", "def linear_model(x, G, b):\n", "    #the fit function again includes a non-zero intercept\n", "    return ######INSERT CODE HERE########\n", "\n", "\n", "n_samp = 100\n", "rgr, rgr_unc = get_rgr_data(m_earth, m_earth_unc, r_earth, r_earth_unc, l, l_unc, n_samp)\n", "lmod = Model(linear_model)\n", "lmod.set_param_hint(name = 'G', value = 1)\n", "lmod.set_param_hint(name = 'b', value = 0.1)\n", "result = lmod.fit(1e-11*rgr, x = pi2dTsqrd, weights = 1/(rgr_unc*1e-11))\n", "print(result.fit_report())\n", "result.plot_fit()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "ba702954", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": [">#### Follow-up 2.3.1a (ungraded)\n", ">\n", ">Compare your calculation of $G$ using the numbers given above to the current accepted value of Newton's gravitational constant. Where did the majority of your error come from?"]}], "metadata": {"celltoolbar": "Tags", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}